---
# An instance of the Pages widget.
# Documentation: https://wowchemy.com/docs/page-builder/
widget: pages

# This file represents a page section.
headless: true

# Order that this section appears on the page.
weight: 20

title: News
subtitle: 

content:
  # Filter on criteria
  filters:
    folders:
      - post
    tag: ''
    category: ''
    publication_type: ''
    author: ''
    exclude_featured: false
    exclude_future: false
    exclude_past: false
  # Choose how many pages you would like to display (0 = all pages)
  count: 5
  # Choose how many pages you would like to offset by
  offset: 0
  # Page order: descending (desc) or ascending (asc) date.
  order: desc

design:
  # Choose a view for the listings:
  view: compact
  columns: '2'
---
- **2023.06**: I will join [Shanghai Artificial Intelligence Laboratory](https://www.shlab.org.cn/) as a research scientist focusing on multi-modal foundation models and their applications.  <span style="color:red">We are hiring full-time researchers working together on this project. I am also looking for talented students targeted to Master or Ph.D. degree, as well as interns. Please drop me an email if you are interested.</span>
- **2023.06**: I gave a talk about Multi-Modal Large Language Models (MLLMs) at [VALSE 2023, Wuxi](http://valser.org/2023/#/), including the data, methodology, benchmark and platform. More details please check the [slides](../uploads/mllm_valse_2023_release_version.pdf).
- **2023.03**: One paper accepted to [CVPR 2023](https://cvpr2023.thecvf.com/).
- **2022.12**: One paper accepted to [NeurIPS 2022](https://nips.cc/Conferences/2022).
- **2022.08**: We are organizing ECCV 2022 (2nd) [DeeperAction Challenge and Workshop](https://deeperaction.github.io/) on detailed video action understanding and anomaly recognition by introducing five new benchmarks.
- **2022.08**: We are hosting ECCV 2022 [OmniBenchmark Challenge](https://zhangyuanhan-ai.github.io/OmniBenchmark/) on evaluation of pre-trained models.
- **2022.07**: <span style="color:red">Winner</span> of [Challenge on RxR-Habitat Vision-and-Language Navigation Competition](https://ai.google.com/research/rxr/habitat).
- **2022.07**: Four papers accepted to [ECCV 2022](https://eccv2022.ecva.net/).
- **2022.04**: We have released [OpenGVLab (书生)](https://opengvlab.shlab.org.cn/).
- **2022.04**: One paper accepted to [IJCAI 2022](https://ijcai-22.org/).
- **2022.03**: Two papers accepted to [ACL 2022](https://www.2022.aclweb.org/).
- **2021.11**: <span style="color:red">A large-scale general vision paradigm [INTERN](https://opengvlab.shlab.org.cn/) is released on [Arxiv](https://arxiv.org/abs/2111.08687)</span>.
- **2021.10**: We are organizing ICCV 2021 (2nd) [SenseHuman Workshop](https://sense-human.github.io/index_2021.html) on human sensing, understanding and synthesis.
- **2021.10**: We are organizing ICCV 2021 (1st) [DeeperAction Workshop](https://deeperaction.github.io/) on detailed video action understanding by introducing three new benchmarks.
- **2021.08**: One paper accepted to <span style="color:red">**IEEE T-PAMI**</span>.
- **2021.07**: We are hosting [ForgeryNet - Face Forgery Analysis Challenge in ICCV 2021](https://competitions.codalab.org/competitions/33386).
- **2021.04**: Two papers accepted to [CVPR 2021](https://cvpr2021.thecvf.com/) (One  <span style="color:red">**Oral**</span> and one Poster).
- **2020.09**: We are hosting [CelebA-Spoof - Face Anti-Spoofing Challenge in ECCV 2020](https://competitions.codalab.org/competitions/26210).
- **2020.08**: <span style="color:red">Winner</span> of [ActivityNet Challenge on Spatio-temporal Action Localization (AVA)](http://activity-net.org/challenges/2020/).
- **2020.07**: 4th Place of [DeepFake Detection Challenge (DFDC)](https://www.kaggle.com/c/deepfake-detection-challenge).
- **2020.07**: One paper accepted to <span style="color:red">**IJCV**</span>.
- **2020.07**: Three papers accepted to [ECCV 2020](https://eccv2020.ecva.net/).
- **2019.09**: Three papers: 1@[ICCV 2019](https://iccv2019.thecvf.com/) on Text-image retrieval, 1@[NeurIPS 2019](https://nips.cc/Conferences/2019) on Layout to image generation, 1@[AAAI 2020](https://aaai.org/Conferences/AAAI-20/) on Dense point cloud completion.
- **2019.06**: <span style="color:green">**[Bug (invalid links) fixed]**</span> [Baidu Disk links](file:///Users/lucasjing/Library/Containers/com.tencent.xinWeChat/Data/Library/Application%20Support/com.tencent.xinWeChat/2.0b4.0.9/0055825b7948b58660b56956863332f3/Handoff/projects/CUHKcrowd_files/cuhk_crowd_dataset_files/CUHKcrowd_dataset_imgTrk_v2.txt) of CUHK Crowd Dataset has been renewed!
- **2019.04**: Four papers accepted to [CVPR 2019](https://cvpr2019.thecvf.com/) (one <span style="color:red">**Oral**</span> and three Posters)
- **2018.07**: Five papers: 3@[ECCV 2018](https://eccv2018.ecva.net/), 1@[ACM MM 2018](https://2022.acmmm.org/2018), 1@[BMVC 2018](http://bmvc2018.org/).
- **2018.04**: Three papers accepted to [CVPR 2018](https://cvpr2018.thecvf.com/) (one <span style="color:red">**Oral**</span> and two Posters)
- **2017.07**: Two papers accepted to [ICCV 2017](https://iccv2017.thecvf.com/).
- **2017.04**: One paper accepted to [CVPR 2017](https://cvpr2017.thecvf.com/)
- **2016.12**: One paper accepted to **IEEE TCSVT**.
- **2016.09**: I am invited to participate in [MSRA Ph.D. Forum 2016](https://www.microsoft.com/en-us/research/blog/top-phd-students-gather-microsoft-research-asia-phd-forum-2016/).
- **2016.06**: Participate in [CVPR 2016 Doctoral Consortium](http://cvpr2016.thecvf.com/program/doctoral_consortium).
- **2016.06**: One paper accepted to **IEEE TCSVT**.
- **2016.04**: One <span style="color:red">**Spotlight**</span> paper accepted to [CVPR 2016](https://cvpr2016.thecvf.com/).
- **2015.04**: One <span style="color:red">**Oral**</span> paper accepted to [CVPR 2015](https://cvpr2015.thecvf.com/).
- **2014.04**: One <span style="color:red">**Oral**</span> paper accepted to [CVPR 2014](https://cvpr2014.thecvf.com/).