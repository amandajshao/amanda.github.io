---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'BlockQNN: Efficient Block-Wise Neural Network Architecture Generation'
subtitle: ''
summary: ''
authors:
- Zhao Zhong
- Zichen Yang
- Boyang Deng
- Junjie Yan
- Wei Wu
- Jing Shao
- Cheng-Lin Liu
tags:
- Acceleration
- AutoML
- Computer architecture
- Convolutional neural network
- Graphics processing units
- Indexes
- Network architecture
- neural architecture search
- Neural networks
- Q-learning
- reinforcement learning
- Task analysis
categories: []
date: '2021-07-01'
lastmod: 2022-08-26T23:26:30+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: ["network"]
publishDate: '2022-08-26T15:26:29.961058Z'
publication_types:
- '2'
abstract: 'Convolutional neural networks have gained a remarkable success in computer
  vision. However, most popular network architectures are hand-crafted and usually
  require expertise and elaborate design. In this paper, we provide a block-wise network
  generation pipeline called BlockQNN which automatically builds high-performance
  networks using the Q-Learning paradigm with epsilon-greedy exploration strategy.
  The optimal network block is constructed by the learning agent which is trained
  to choose component layers sequentially. We stack the block to construct the whole
  auto-generated network. To accelerate the generation process, we also propose a
  distributed asynchronous framework and an early stop strategy. The block-wise generation
  brings unique advantages: (1) it yields state-of-the-art results in comparison to
  the hand-crafted networks on image classification, particularly, the best network
  generated by BlockQNN achieves 2.35 percent top-1 error rate on CIFAR-10. (2) it
  offers tremendous reduction of the search space in designing networks, spending
  only 3 days with 32 GPUs. A faster version can yield a comparable result with only
  1 GPU in 20 hours. (3) it has strong generalizability in that the network built
  on CIFAR also performs well on the larger-scale dataset. The best network achieves
  very competitive accuracy of 82.0 percent top-1 and 96.0 percent top-5 on ImageNet.'
publication: 'IEEE Transactions on Pattern Analysis and Machine Intelligence (**IEEE T-PAMI**), 2021'
doi: 10.1109/TPAMI.2020.2969193
---
