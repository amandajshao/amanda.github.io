---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially
  Labeled Data'
subtitle: ''
summary: ''
authors:
- Xihui Liu
- Hongsheng Li
- Jing Shao
- Dapeng Chen
- Xiaogang Wang
author_notes:
- 
- "Coresponding author"
tags: ["Image Captioning", "Partially Labelled Data"]
categories: ["Vision and Language"]
date: '2018-09-08'
lastmod: 2022-08-26T23:26:31+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: ["text-image-retrieval-and-grounding"]
publishDate: '2022-08-26T15:26:30.989325Z'
publication_types:
- '1'
abstract: 'The aim of image captioning is to generate captions by machine to describe
  image contents. Despite many eﬀorts, generating discriminative captions for images
  remains non-trivial. Most traditional approaches imitate the language structure
  patterns, thus tend to fall into a stereotype of replicating frequent phrases or
  sentences and neglect unique aspects of each image. In this work, we propose an
  image captioning framework with a self-retrieval module as training guidance, which
  encourages generating discriminative captions. It brings unique advantages: (1)
  the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness
  to assure the quality of generated captions. (2) The correspondence between generated
  captions and images are naturally incorporated in the generation process without
  human annotations, and hence our approach could utilize a large amount of unlabeled
  images to boost captioning performance with no additional annotations. We demonstrate
  the eﬀectiveness of the proposed retrievalguided method on COCO and Flickr30k captioning
  datasets, and show its superior captioning performance with more discriminative
  captions.'
publication: 'European Conference on Computer Vision (**ECCV**), 2018'
doi: 10.1007/978-3-030-01267-0_21

url_pdf: "https://arxiv.org/abs/1803.08314"
url_code: 
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:

---
