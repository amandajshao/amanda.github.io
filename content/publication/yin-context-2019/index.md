---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Context and Attribute Grounded Dense Captioning
subtitle: ''
summary: ''
authors:
- Guojun Yin
- Lu Sheng
- Bin Liu
- Nenghai Yu
- Xiaogang Wang
- Jing Shao
author_notes:
-
-
- "Corresponding author"
tags: []
categories: []
date: '2019-06-01'
lastmod: 2022-08-26T23:26:28+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: ["text-image-retrieval-and-grounding"]
publishDate: '2022-08-26T15:26:28.558183Z'
publication_types:
- '1'
abstract: Dense captioning aims at simultaneously localizing semantic regions and
  describing these regions-of-interest (ROIs) with short phrases or sentences in natural
  language. Previous studies have shown remarkable progresses, but they are often
  vulnerable to the aperture problem that a caption generated by the features inside
  one ROI lacks contextual coherence with its surrounding context in the input image.
  In this work, we investigate contextual reasoning based on multi-scale message propagations
  from the neighboring contents to the target ROIs. To this end, we design a novel
  end-to-end context and attribute grounded dense captioning framework consisting
  of 1) a contextual visual mining module and 2) a multi-level attribute grounded
  description generation module. Knowing that captions often co-occur with the linguistic
  attributes (such as who, what and where), we also incorporate an auxiliary supervision
  from hierarchical linguistic attributes to augment the distinctiveness of the learned
  captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate
  the superiority of the proposed model in comparison to the state-of-the-art methods.
publication: '*2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR)*'
doi: 10.1109/CVPR.2019.00640

url_pdf: "https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.pdf"

---
