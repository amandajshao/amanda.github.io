---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Improving Referring Expression Grounding With Cross-Modal Attention-Guided
  Erasing
subtitle: ''
summary: ''
authors:
- Xihui Liu
- Zihao Wang
- Jing Shao
- Xiaogang Wang
- Hongsheng Li
tags: []
categories: []
date: '2019-06-01'
lastmod: 2022-08-26T23:26:30+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: ["text-image-retrieval-and-grounding"]
publishDate: '2022-08-26T15:26:30.344414Z'
publication_types:
- '1'
abstract: Referring expression grounding aims at locating certain objects or persons
  in an image with a referring expression, where the key challenge is to comprehend
  and align various types of information from visual and textual domain, such as visual
  attributes, location and interactions with surrounding regions. Although the attention
  mechanism has been successfully applied for cross-modal alignments, previous attention
  models focus on only the most dominant features of both modalities, and neglect
  the fact that there could be multiple comprehensive textual-visual correspondences
  between images and referring expressions. To tackle this issue, we design a novel
  cross-modal attention-guided erasing approach, where we discard the most dominant
  information from either textual or visual domains to generate difÔ¨Åcult training
  samples online, and to drive the model to discover complementary textual-visual
  correspondences. Extensive experiments demonstrate the effectiveness of our proposed
  method, which achieves state-of-the-art performance on three referring expression
  grounding datasets.
publication: 'IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (**CVPR**), 2019'
doi: 10.1109/CVPR.2019.00205

url_pdf: "https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Improving_Referring_Expression_Grounding_With_Cross-Modal_Attention-Guided_Erasing_CVPR_2019_paper.pdf"
url_code: "https://github.com/xh-liu/CM-Erase-REG"
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:

---
