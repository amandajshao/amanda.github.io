<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Jing Shao"><meta name=description content="Research Director"><link rel=alternate hreflang=en-us href=https://amandajshao.github.io/publication-type/1/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.e16b071c3437dd38fe4bc6049c3c2fff.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=alternate href=/publication-type/1/index.xml type=application/rss+xml title="Jing Shao (邵婧)"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://amandajshao.github.io/publication-type/1/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Jing Shao (邵婧)"><meta property="og:url" content="https://amandajshao.github.io/publication-type/1/"><meta property="og:title" content="1 | Jing Shao (邵婧)"><meta property="og:description" content="Research Director"><meta property="og:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-07-16T00:00:00+00:00"><title>1 | Jing Shao (邵婧)</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>1</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/vedaldi-powering-2020/>Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy</a></div><a href=/publication/vedaldi-powering-2020/ class=summary-link><div class=article-style>One-shot NAS method has attracted much interest from the research community due to its remarkable training eﬃciency and capacity to …</div></a><div class="stream-meta article-metadata"><div><span>Ronghao Guo</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Chen Lin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Chuming Li</span>, <span>Keyu Tian</span>, <span>Ming Sun</span>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Junjie Yan</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590613.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-powering-2020/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58568-6_37 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/vedaldi-thinking-2020/>Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues</a></div><a href=/publication/vedaldi-thinking-2020/ class=summary-link><div class=article-style>As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of …</div></a><div class="stream-meta article-metadata"><div><span>Yuyang Qian</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Guojun Yin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution, Corresponding author"></i>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Zixuan Chen</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590613.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-thinking-2020/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58610-2_6 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/liu-morphing-2020/>Morphing and Sampling Network for Dense Point Cloud Completion</a></div><a href=/publication/liu-morphing-2020/ class=summary-link><div class=article-style>3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention …</div></a><div class="stream-meta article-metadata"><div><span>Minghua Liu</span>, <span>Lu Sheng</span>, <span>Sheng Yang</span>, <span>Jing Shao</span>, <span>Shi-Min Hu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ojs.aaai.org/index.php/AAAI/article/view/6827 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-morphing-2020/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Colin97/MSN-Point-Cloud-Completion target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://drive.google.com/drive/folders/1oYOT8fTUyj6_db9f4cYiKFqP9LPEG5nH target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1609/aaai.v34i07.6827 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/liu-learning-2019/>Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</a></div><a href=/publication/liu-learning-2019/ class=summary-link><div class=article-style>Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional …</div></a><div class="stream-meta article-metadata"><div><span>Xihui Liu</span>, <span>Guojun Yin</span>, <span>Jing Shao</span>, <span>Xiaogang Wang</span>, <span>Hongsheng Li</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1910.06809.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-learning-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/xh-liu/CC-FPSE target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://drive.google.com/file/d/1m3JKSUotm6sRImak_qjwBMtMtd037XeK/view target=_blank rel=noopener>Slides</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/wang-camp-2019/>CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</a></div><a href=/publication/wang-camp-2019/ class=summary-link><div class=article-style>Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently …</div></a><div class="stream-meta article-metadata"><div><span>Zihao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xihui Liu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Hongsheng Li</span>, <span>Lu Sheng</span>, <span>Junjie Yan</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-camp-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ZihaoWang-CV/CAMP_iccv19 target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICCV.2019.00586 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/yin-context-2019/>Context and Attribute Grounded Dense Captioning</a></div><a href=/publication/yin-context-2019/ class=summary-link><div class=article-style>Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases …</div></a><div class="stream-meta article-metadata"><div><span>Guojun Yin</span>, <span>Lu Sheng</span>, <span>Bin Liu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Nenghai Yu</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/yin-context-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00640 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/liu-improving-2019/>Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing</a></div><a href=/publication/liu-improving-2019/ class=summary-link><div class=article-style>Referring expression grounding aims at locating certain objects or persons in an image with a referring expression, where the key …</div></a><div class="stream-meta article-metadata"><div><span>Xihui Liu</span>, <span>Zihao Wang</span>, <span>Jing Shao</span>, <span>Xiaogang Wang</span>, <span>Hongsheng Li</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Improving_Referring_Expression_Grounding_With_Cross-Modal_Attention-Guided_Erasing_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-improving-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/xh-liu/CM-Erase-REG target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00205 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/yin-semantics-2019/>Semantics Disentangling for Text-To-Image Generation</a></div><a href=/publication/yin-semantics-2019/ class=summary-link><div class=article-style>Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses …</div></a><div class="stream-meta article-metadata"><div><span>Guojun Yin</span>, <span>Bin Liu</span>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Nenghai Yu</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Semantics_Disentangling_for_Text-To-Image_Generation_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/yin-semantics-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00243 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/pan-video-2019/>Video Generation From Single Semantic Label Map</a></div><a href=/publication/pan-video-2019/ class=summary-link><div class=article-style>This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance …</div></a><div class="stream-meta article-metadata"><div><span>Junting Pan</span>, <span>Chengyu Wang</span>, <span>Xu Jia</span>, <span>Jing Shao</span>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Junjie Yan</span>, <span>Xiaogang Wang</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Pan_Video_Generation_From_Single_Semantic_Label_Map_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pan-video-2019/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/junting/seg2vid target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00385 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/liu-multi-label-2018/>Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection</a></div><a href=/publication/liu-multi-label-2018/ class=summary-link><div class=article-style>Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the …</div></a><div class="stream-meta article-metadata"><div><span>Yongcheng Liu</span>, <span>Lu Sheng</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Junjie Yan</span>, <span>Shiming Xiang</span>, <span>Chunhong Pan</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1809.05884 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-multi-label-2018/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Yochengliu/MLIC-KD-WSD target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3240508.3240567 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/publication-type/1/>&#171;</a></li><li class=page-item><a class=page-link href=/publication-type/1/page/3/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Jing Shao. Last updated Aug 30, 2022.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.2621b0c3d1130efc0b22bddd510b0848.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.85290d887e7fcdd400ccb3ffb9bcd3e3.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>