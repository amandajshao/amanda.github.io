<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Jing Shao"><meta name=description content="Research Director"><link rel=alternate hreflang=en-us href=https://amandajshao.github.io/publication-type/1/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.e16b071c3437dd38fe4bc6049c3c2fff.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=alternate href=/publication-type/1/index.xml type=application/rss+xml title="Jing Shao (邵婧)"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://amandajshao.github.io/publication-type/1/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Jing Shao (邵婧)"><meta property="og:url" content="https://amandajshao.github.io/publication-type/1/"><meta property="og:title" content="1 | Jing Shao (邵婧)"><meta property="og:description" content="Research Director"><meta property="og:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-07-16T00:00:00+00:00"><title>1 | Jing Shao (邵婧)</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>1</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/he-x-learner-2022/>X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation</a></div><a href=/publication/he-x-learner-2022/ class=summary-link><div class=article-style>In computer vision, pre-training models based on largescale supervised learning have been proven effective over the past few years. …</div></a><div class="stream-meta article-metadata"><div><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jianing Teng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Wang Kun</span>, <span>Zhenfei Yin</span>, <span>Lu Sheng</span>, <span>Ziwei Liu</span>, <span>Yu Qiao</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2203.08764 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-x-learner-2022/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/he-x-learner-2022/><img src=/publication/he-x-learner-2022/featured_hu52126672e44ff1954a56b3434fc7e045_303648_150x0_resize_q75_h2_lanczos_3.webp height=33 width=150 alt="X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/zhang-benchmarking-2022/>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</a></div><a href=/publication/zhang-benchmarking-2022/ class=summary-link><div class=article-style>Though impressive performance has been achieved in specific visual realms (e.g. faces, dogs, and places), an omni-vision representation …</div></a><div class="stream-meta article-metadata"><div><span>Yuanhan Zhang</span>, <span>Zhenfei Yin</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2207.07106 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-benchmarking-2022/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/zhang-benchmarking-2022/><img src=/publication/zhang-benchmarking-2022/featured_hu0a02987c1b5bc970b81bbe1882c329b7_1377794_150x0_resize_q75_h2_lanczos.webp height=111 width=150 alt="Benchmarking Omni-Vision Representation through the Lens of Visual Realms" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/wang-repre-2022/>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</a></div><div class="stream-meta article-metadata"><div><span>Luya Wang</span>, <span>Feng Liang</span>, <span>Yangguang Li</span>, <span>Honggang Zhang</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ijcai.org/proceedings/2022/0200.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-repre-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.24963/ijcai.2022/200 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/ma-mmekg-2022/>MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities</a></div><a href=/publication/ma-mmekg-2022/ class=summary-link><div class=article-style>Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge …</div></a><div class="stream-meta article-metadata"><div><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mukai Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Meiqi Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xinze Li</span>, <span>Wenqi Sun</span>, <span>Kunquan Deng</span>, <span>Kun Wang</span>, <span>Aixin Sun</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-demo.23.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-mmekg-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-demo.23 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/ma-mmekg-2022/><img src=/publication/ma-mmekg-2022/featured_hu3d61ca839dc9438c93226ec7c8957810_256537_150x0_resize_q75_h2_lanczos_3.webp height=56 width=150 alt="MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/ma-prompt-2022/>Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction</a></div><a href=/publication/ma-prompt-2022/ class=summary-link><div class=article-style>In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction …</div></a><div class="stream-meta article-metadata"><div><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Mukai Li</span>, <span>Meiqi Chen</span>, <span>Kun Wang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-long.466.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-prompt-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mayubo2333/PAIE target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-long.466 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/ma-prompt-2022/><img src=/publication/ma-prompt-2022/featured_hufc3ee82aef12822ef8a9452401ba7e58_365608_150x0_resize_q75_h2_lanczos_3.webp height=53 width=150 alt="Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/li-supervision-2022/>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</a></div><a href=/publication/li-supervision-2022/ class=summary-link><div class=article-style>Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot …</div></a><div class="stream-meta article-metadata"><div><span>Yangguang Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Feng Liang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yufeng Cui</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span>, <span>Fengwei Yu</span>, <span>Junjie Yan</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=zq1iJkNk3uN" target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/li-supervision-2022/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/pan-actor-context-actor-2021/>Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization</a></div><div class="stream-meta article-metadata"><div><span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mike Zheng Shou</span>, <span>Yu Liu</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_Actor-Context-Actor_Relation_Network_for_Spatio-Temporal_Action_Localization_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pan-actor-context-actor-2021/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Siyu-C/ACAR-Net target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR46437.2021.00053 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/he-forgerynet-2021/>ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</a></div><a href=/publication/he-forgerynet-2021/ class=summary-link><div class=article-style>The rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and …</div></a><div class="stream-meta article-metadata"><div><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Bei Gan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yichun Zhou</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Guojun Yin</span>, <span>Luchuan Song</span>, <span>Lu Sheng</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-forgerynet-2021/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://yinanhe.github.io/projects/forgerynet.html#download target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR46437.2021.00434 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/he-forgerynet-2021/><img src=/publication/he-forgerynet-2021/featured_hu191f8586937ae967706b6daa8badb15b_2314096_150x0_resize_q75_h2_lanczos_3.webp height=62 width=150 alt="ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/vedaldi-celeba-spoof-2020/>CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations</a></div><a href=/publication/vedaldi-celeba-spoof-2020/ class=summary-link><div class=article-style><strong>CelebA-Spoof</strong> is a large-scale face anti-spoofing dataset that has 625,537 images from 10,177 subjects, which includes 43 rich attributes on face, illumination, environment and spoof types. Live images are selected from the CelebA dataset. We collect and annotate spoof images for CelebA-Spoof. Among 43 rich attributes, 40 attributes belong to live images including all facial components and accessories such as skin, nose, eyes, eyebrows, lip, hair, hat, eyeglass. 3 attributes belong to spoof images including spoof types, environments and illumination conditions. <strong>CelebA-Spoof</strong> can be used to train and evaluate algorithms of face anti-spoofing, face presentation attacks, and robustness/security research.</div></a><div class="stream-meta article-metadata"><div><span>Yuanhan Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zhenfei Yin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yidong Li</span>, <span>Guojun Yin</span>, <span>Junjie Yan</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570069.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-celeba-spoof-2020/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ZhangYuanhan-AI/CelebA-Spoof target=_blank rel=noopener>Dataset</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=A7XjSg5srvI&t=4s" target=_blank rel=noopener>Video</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58610-2_5 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/vedaldi-celeba-spoof-2020/><img src=/publication/vedaldi-celeba-spoof-2020/featured_hub1073de3746bfac5ba178b3ed1d12f7c_5426219_150x0_resize_q75_h2_lanczos_3.webp height=66 width=150 alt="CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/vedaldi-learning-2020/>Learning Connectivity of Neural Networks from a Topological Perspective</a></div><a href=/publication/vedaldi-learning-2020/ class=summary-link><div class=article-style>Seeking eﬀective neural networks is a critical and practical ﬁeld in deep learning. Besides designing the depth, type of convolution, …</div></a><div class="stream-meta article-metadata"><div><span>Kun Yuan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Quanquan Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jing Shao</span>, <span>Junjie Yan</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660732.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-learning-2020/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58589-1_44 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/publication-type/1/page/2/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Jing Shao. Last updated Aug 30, 2022.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.54dd6e4d8f2e4b1d098381b57f18dd83.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>