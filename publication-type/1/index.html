<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.5691485ce8e88de823038b8e3d90ec13.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Jing Shao"><meta name=description content="Research Scientist"><link rel=alternate hreflang=en-us href=https://amandajshao.github.io/publication-type/1/><link rel=canonical href=https://amandajshao.github.io/publication-type/1/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Jing Shao (邵婧)"><meta property="og:url" content="https://amandajshao.github.io/publication-type/1/"><meta property="og:title" content="1 | Jing Shao (邵婧)"><meta property="og:description" content="Research Scientist"><meta property="og:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-06-20T00:00:00+00:00"><link rel=alternate href=/publication-type/1/index.xml type=application/rss+xml title="Jing Shao (邵婧)"><title>1 | Jing Shao (邵婧)</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>1</h1></div><div class=universal-wrapper><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/siamese-detr/>Siamese DETR</a></div><a href=/publication/siamese-detr/ class=summary-link><div class=article-style>Recent self-supervised methods are mainly designed for representation learning with the base model, eg, ResNets or ViTs. They cannot be …</div></a><div class="stream-meta article-metadata"><div><span>Zeren Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Wei Li</span>, <span>Jianing Teng</span>, <span>Kun Wang</span>, <span>Jing Shao</span>, <span>Chen Change Loy</span>, <span>Lu Sheng</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Siamese_DETR_CVPR_2023_paper.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Zx55/SiameseDETR target=_blank rel=noopener>Code</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/st-adapter/>ST-Adapter: Parameter-efficient Image-to-Video Transfer Learning</a></div><a href=/publication/st-adapter/ class=summary-link><div class=article-style>Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due …</div></a><div class="stream-meta article-metadata"><div><span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Ziyi Lin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Xiatian Zhu</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.neurips.cc/paper_files/paper/2022/file/a92e9165b22d4456fc6d87236e04c266-Paper-Conference.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/linziyi96/st-adapter target=_blank rel=noopener>Code</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/he-x-learner-2022/>X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation</a></div><a href=/publication/he-x-learner-2022/ class=summary-link><div class=article-style>In computer vision, pre-training models based on largescale supervised learning have been proven effective over the past few years. …</div></a><div class="stream-meta article-metadata"><div><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jianing Teng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Wang Kun</span>, <span>Zhenfei Yin</span>, <span>Lu Sheng</span>, <span>Ziwei Liu</span>, <span>Yu Qiao</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2203.08764 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-x-learner-2022/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/he-x-learner-2022/><img src=/publication/he-x-learner-2022/featured_hu52126672e44ff1954a56b3434fc7e045_303648_7f092f6d8eabc84d1ec48a8f0f1955a8.webp height=33 width=150 alt="X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/zhang-benchmarking-2022/>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</a></div><a href=/publication/zhang-benchmarking-2022/ class=summary-link><div class=article-style>Though impressive performance has been achieved in specific visual realms (e.g. faces, dogs, and places), an omni-vision representation …</div></a><div class="stream-meta article-metadata"><div><span>Yuanhan Zhang</span>, <span>Zhenfei Yin</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2207.07106 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-benchmarking-2022/cite.bib>Cite</a></div></div><div class=ml-3><a href=/publication/zhang-benchmarking-2022/><img src=/publication/zhang-benchmarking-2022/featured_hu0a02987c1b5bc970b81bbe1882c329b7_1377794_8e0f01517bdc7af9d8cd984d006821fe.webp height=111 width=150 alt="Benchmarking Omni-Vision Representation through the Lens of Visual Realms" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/wang-repre-2022/>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</a></div><div class="stream-meta article-metadata"><div><span>Luya Wang</span>, <span>Feng Liang</span>, <span>Yangguang Li</span>, <span>Honggang Zhang</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ijcai.org/proceedings/2022/0200.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-repre-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.24963/ijcai.2022/200 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/ma-mmekg-2022/>MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities</a></div><a href=/publication/ma-mmekg-2022/ class=summary-link><div class=article-style>Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge …</div></a><div class="stream-meta article-metadata"><div><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mukai Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Meiqi Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xinze Li</span>, <span>Wenqi Sun</span>, <span>Kunquan Deng</span>, <span>Kun Wang</span>, <span>Aixin Sun</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-demo.23.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-mmekg-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-demo.23 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/ma-mmekg-2022/><img src=/publication/ma-mmekg-2022/featured_hu3d61ca839dc9438c93226ec7c8957810_256537_0f78bb9af71a4f656418d2f07c9eac03.webp height=56 width=150 alt="MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/ma-prompt-2022/>Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction</a></div><a href=/publication/ma-prompt-2022/ class=summary-link><div class=article-style>In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction …</div></a><div class="stream-meta article-metadata"><div><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Mukai Li</span>, <span>Meiqi Chen</span>, <span>Kun Wang</span>, <span>Jing Shao</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-long.466.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-prompt-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mayubo2333/PAIE target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-long.466 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/ma-prompt-2022/><img src=/publication/ma-prompt-2022/featured_hufc3ee82aef12822ef8a9452401ba7e58_365608_fc2f4ee36adae8517da3b29980aa6456.webp height=53 width=150 alt="Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction" loading=lazy></a></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/li-supervision-2022/>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</a></div><a href=/publication/li-supervision-2022/ class=summary-link><div class=article-style>Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot …</div></a><div class="stream-meta article-metadata"><div><span>Yangguang Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Feng Liang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yufeng Cui</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span>, <span>Fengwei Yu</span>, <span>Junjie Yan</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=zq1iJkNk3uN" target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/li-supervision-2022/cite.bib>Cite</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/pan-actor-context-actor-2021/>Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization</a></div><div class="stream-meta article-metadata"><div><span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mike Zheng Shou</span>, <span>Yu Liu</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_Actor-Context-Actor_Relation_Network_for_Spatio-Temporal_Action_Localization_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pan-actor-context-actor-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Siyu-C/ACAR-Net target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR46437.2021.00053 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3></div></div><div class="media stream-item view-compact"><div class=media-body><div class="section-subheading article-title mb-0 mt-0"><a href=/publication/he-forgerynet-2021/>ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</a></div><a href=/publication/he-forgerynet-2021/ class=summary-link><div class=article-style>The rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and …</div></a><div class="stream-meta article-metadata"><div><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Bei Gan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yichun Zhou</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Guojun Yin</span>, <span>Luchuan Song</span>, <span>Lu Sheng</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span></div></div><div class=btn-links><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-forgerynet-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://yinanhe.github.io/projects/forgerynet.html#download target=_blank rel=noopener>Dataset
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR46437.2021.00434 target=_blank rel=noopener>DOI</a></div></div><div class=ml-3><a href=/publication/he-forgerynet-2021/><img src=/publication/he-forgerynet-2021/featured_hu191f8586937ae967706b6daa8badb15b_2314096_1fb46d9c06f7db2c4e66ba126edee9d2.webp height=62 width=150 alt="ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis" loading=lazy></a></div></div><nav class=mt-1><ul class="pagination justify-content-center"><li class=page-item><a class=page-link href=/publication-type/1/page/2/>&#187;</a></li></ul></nav></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Jing Shao. Last updated Aug 30, 2022.</p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.391d344a129df56f7ad674c2c2ed04e8.js></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.7f5ebaff62ae468cff8bb3dd1337bb9b.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>