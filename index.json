[{"authors":null,"categories":null,"content":"[News!] Jing Shao is currently at Shanghai Artificial Intelligence Laboratory as a research scientist focusing on multi-modal foundation models and agents, with special interests in understanding various properties of current models beyond their accuracy, such as explainability, robustness, safety and generalization, towards the reliableness of AI systems. She is also an Adjunt Ph.D. supervisor at the Shanghai Jiao Tong University, and a Co-PI of S-Lab in Nanyang Technological University. She received her Ph.D. (2016) in Electronic Engineering from The Chinese University of Hong Kong (CUHK), supervised by Prof. Xiaogang Wang, and work closely with Prof. Chen Change Loy and the Multimedia Lab (MMLab) led by Prof. Xiaoou Tang.\nShe has published 40+ peer-reviewed articles (including 20 first/co-first/corresponding author papers) in top-tier conferences and journals such as TPAMI, IJCV, ICML, ICLR, NeurIPS and CVPR, with 7300+ citations in Google Scholar. She serves as the reviewer of IJCV, T-PAMI, T-CSVT, T-MM, T-ITS, and CVIU, and reviewed CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI, IJCAI, and ACM MM.\nWe are hiring Full-time Researchers/Interns working together on safety/robustness/explainablity of generative models and agents. I am also looking for talented students targeted to Master or Ph.D. degree. Please drop me an email if you are interested.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"[News!] Jing Shao is currently at Shanghai Artificial Intelligence Laboratory as a research scientist focusing on multi-modal foundation models and agents, with special interests in understanding various properties of current models beyond their accuracy, such as explainability, robustness, safety and generalization, towards the reliableness of AI systems.","tags":null,"title":"Jing Shao","type":"authors"},{"authors":["Zeren Chen","Gengshi Huang","Wei Li","Jianing Teng","Kun Wang","Jing Shao","Chen Change Loy","Lu Sheng"],"categories":[],"content":"","date":1687219200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687219200,"objectID":"2cfef8ca677cf88aafda619ed3519be9","permalink":"https://amandajshao.github.io/publication/siamese-detr/","publishdate":"2023-06-13T15:24:25+08:00","relpermalink":"/publication/siamese-detr/","section":"publication","summary":"Recent self-supervised methods are mainly designed for representation learning with the base model, eg, ResNets or ViTs. They cannot be easily transferred to DETR, with task-specific Transformer modules. In this work, we present Siamese DETR, a Siamese self-supervised pretraining approach for the Transformer architecture in DETR. We consider learning view-invariant and detection-oriented representations simultaneously through two complementary tasks, ie, localization and discrimination, in a novel multi-view learning framework. Two self-supervised pretext tasks are designed:(i) Multi-View Region Detection aims at learning to localize regions-of-interest between augmented views of the input, and (ii) Multi-View Semantic Discrimination attempts to improve object-level discrimination for each region. The proposed Siamese DETR achieves state-of-the-art transfer performance on COCO and PASCAL VOC detection using different DETR variants in all setups. Code is available at https://github.com/Zx55/SiameseDETR.","tags":[],"title":"Siamese DETR","type":"publication"},{"authors":["Junting Pan","Ziyi Lin","Xiatian Zhu","Jing Shao","Hongsheng Li"],"categories":[],"content":"","date":1670284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670284800,"objectID":"8e589651ee1beaa7a16703ca7695fa04","permalink":"https://amandajshao.github.io/publication/st-adapter/","publishdate":"2023-06-13T15:13:44+08:00","relpermalink":"/publication/st-adapter/","section":"publication","summary":"Capitalizing on large pre-trained models for various downstream tasks of interest have recently emerged with promising performance. Due to the ever-growing model size, the standard full fine-tuning based task adaptation strategy becomes prohibitively costly in terms of model training and storage. This has led to a new research direction in parameter-efficient transfer learning. However, existing attempts typically focus on downstream tasks from the same modality (eg, image understanding) of the pre-trained model. This creates a limit because in some specific modalities,(eg, video understanding) such a strong pre-trained model with sufficient knowledge is less or not available. In this work, we investigate such a novel cross-modality transfer learning setting, namely parameter-efficient image-to-video transfer learning. To solve this problem, we propose a new Spatio-Temporal Adapter (ST-Adapter) for parameter-efficient fine-tuning per video task. With a built-in spatio-temporal reasoning capability in a compact design, ST-Adapter enables a pre-trained image model without temporal knowledge to reason about dynamic video content at a small~ 8% per-task parameter cost, requiring approximately 20 times fewer updated parameters compared to previous work. Extensive experiments on video action recognition tasks show that our ST-Adapter can match or even outperform the strong full fine-tuning strategy and state-of-the-art video models, whilst enjoying the advantage of parameter efficiency.","tags":[],"title":"ST-Adapter: Parameter-efficient Image-to-Video Transfer Learning","type":"publication"},{"authors":["Yuanhan Zhang","Qinghong Sun","Yichun Zhou","Zexin He","Zhenfei Yin","Kun Wang","Lu Sheng","Yu Qiao","Jing Shao","Ziwei Liu"],"categories":["General Representation Learning"],"content":"","date":1661299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"4547e2d4347caeed89fc203439aa36bf","permalink":"https://amandajshao.github.io/publication/zhang-bamboo-2022/","publishdate":"2022-08-26T15:26:32.398269Z","relpermalink":"/publication/zhang-bamboo-2022/","section":"publication","summary":"","tags":[],"title":"Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy","type":"publication"},{"authors":["Yinan He","Gengshi Huang","Siyu Chen","Jianing Teng","Wang Kun","Zhenfei Yin","Lu Sheng","Ziwei Liu","Yu Qiao","Jing Shao"],"categories":[],"content":"","date":1657929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"bc9d6d090767fcadca40fb32a6600f87","permalink":"https://amandajshao.github.io/publication/he-x-learner-2022/","publishdate":"2022-08-26T15:26:32.526953Z","relpermalink":"/publication/he-x-learner-2022/","section":"publication","summary":"In computer vision, pre-training models based on largescale supervised learning have been proven effective over the past few years. However, existing works mostly focus on learning from individual task with single data source (e.g., ImageNet for classification or COCO for detection). This restricted form limits their generalizability and usability due to the lack of vast semantic information from various tasks and data sources. Here, we demonstrate that jointly learning from heterogeneous tasks and multiple data sources contributes to universal visual representation, leading to better transferring results of various downstream tasks. Thus, learning how to bridge the gaps among different tasks and data sources is the key, but it still remains an open question. In this work, we propose a representation learning framework called X-Learner, which learns the universal feature of multiple vision tasks supervised by various sources, with expansion and squeeze stage: 1) Expansion Stage: X-Learner learns the task-specific feature to alleviate task interference and enrich the representation by reconciliation layer. 2) Squeeze Stage: X-Learner condenses the model to a reasonable size and learns the universal and generalizable representation for various tasks transferring. Extensive experiments demonstrate that X-Learner achieves strong performance on different tasks without extra annotations, modalities and computational costs compared to existing representation learning methods. Notably, a single X-Learner model shows remarkable gains of 3.0%, 3.3% and 1.8% over current pretrained models on 12 downstream datasets for classification, object detection and semantic segmentation.","tags":[],"title":"X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation","type":"publication"},{"authors":["Yuanhan Zhang","Zhenfei Yin","Jing Shao","Ziwei Liu"],"categories":[],"content":"","date":1657756800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"a126e4f37f119d7295288e537143757e","permalink":"https://amandajshao.github.io/publication/zhang-benchmarking-2022/","publishdate":"2022-08-26T15:26:33.293267Z","relpermalink":"/publication/zhang-benchmarking-2022/","section":"publication","summary":"Though impressive performance has been achieved in specific visual realms (e.g. faces, dogs, and places), an omni-vision representation generalizing to many natural visual domains is highly desirable. But, existing benchmarks are biased and inefficient to evaluate the omni-vision representation -- these benchmarks either only include several specific realms, or cover most realms at the expense of subsuming numerous datasets that have extensive realm overlapping. In this paper, we propose Omni-Realm Benchmark (OmniBenchmark). It includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images. Without semantic overlapping, these datasets cover most visual realms comprehensively and meanwhile efficiently. In addition, we propose a new supervised contrastive learning framework, namely Relational Contrastive learning (ReCo), for a better omni-vision representation. Beyond pulling two instances from the same concept closer -- the typical supervised contrastive learning framework -- ReCo also pulls two instances from the same semantic realm closer, encoding the semantic relation between concepts, and facilitating omni-vision representation learning. We benchmark ReCo and other advances in omni-vision representation studies that are different in architectures (from CNNs to transformers) and in learning paradigms (from supervised learning to self-supervised learning) on OmniBenchmark. We illustrate the superior of ReCo to other supervised contrastive learning methods and reveal multiple practical observations to facilitate future research.","tags":[],"title":"Benchmarking Omni-Vision Representation through the Lens of Visual Realms","type":"publication"},{"authors":["Luya Wang","Feng Liang","Yangguang Li","Honggang Zhang","Wanli Ouyang","Jing Shao"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"70549c7025f60aef73ab2902f3b3c78d","permalink":"https://amandajshao.github.io/publication/wang-repre-2022/","publishdate":"2022-08-26T15:26:29.831965Z","relpermalink":"/publication/wang-repre-2022/","section":"publication","summary":"","tags":[],"title":"RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training","type":"publication"},{"authors":["Dong An","Zun Wang","Yangguang Li","Yi Wang","Yicong Hong","Yan Huang","Liang Wang","Jing Shao"],"categories":[],"content":"","date":1655942400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"a22534eae6381cf3411613d0ec188bef","permalink":"https://amandajshao.github.io/publication/an-1-st-2022/","publishdate":"2022-08-26T15:26:33.041864Z","relpermalink":"/publication/an-1-st-2022/","section":"publication","summary":"","tags":[],"title":"1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)","type":"publication"},{"authors":["Yubo Ma","Zehao Wang","Mukai Li","Yixin Cao","Meiqi Chen","Xinze Li","Wenqi Sun","Kunquan Deng","Kun Wang","Aixin Sun","Jing Shao"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"f257d00151dfa08b9afa5dd86d0fc5d8","permalink":"https://amandajshao.github.io/publication/ma-mmekg-2022/","publishdate":"2022-08-26T15:26:29.453386Z","relpermalink":"/publication/ma-mmekg-2022/","section":"publication","summary":"Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images. To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system. We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG. To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels.","tags":[],"title":"MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities","type":"publication"},{"authors":["Yubo Ma","Zehao Wang","Yixin Cao","Mukai Li","Meiqi Chen","Kun Wang","Jing Shao"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"7206c89bccb68c6d07566fe1db2b4421","permalink":"https://amandajshao.github.io/publication/ma-prompt-2022/","publishdate":"2022-08-26T15:26:29.584431Z","relpermalink":"/publication/ma-prompt-2022/","section":"publication","summary":"In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5% and 2.3% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.","tags":[],"title":"Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction","type":"publication"},{"authors":["Meiqi Chen","Yixin Cao","Kunquan Deng","Mukai Li","Kun Wang","Jing Shao","Yan Zhang"],"categories":[],"content":"","date":1649980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"408ed33fdf693b9fd9a658115ea43546","permalink":"https://amandajshao.github.io/publication/chen-ergo-2022/","publishdate":"2022-08-26T15:26:32.78989Z","relpermalink":"/publication/chen-ergo-2022/","section":"publication","summary":"Document-level Event Causality Identification (DECI) aims to identify causal relations between event pairs in a document. It poses a great challenge of across-sentence reasoning without clear causal indicators. In this paper, we propose a novel Event Relational Graph TransfOrmer (ERGO) framework for DECI, which improves existing state-of-the-art (SOTA) methods upon two aspects. First, we formulate DECI as a node classification problem by constructing an event relational graph, without the needs of prior knowledge or tools. Second, ERGO seamlessly integrates event-pair relation classification and global inference, which leverages a Relational Graph Transformer (RGT) to capture the potential causal chain. Besides, we introduce edge-building strategies and adaptive focal loss to deal with the massive false positives caused by common spurious correlation. Extensive experiments on two benchmark datasets show that ERGO significantly outperforms previous SOTA methods (13.1% F1 gains on average). We have conducted extensive quantitative analysis and case studies to provide insights for future research directions (Section 4.8).","tags":["Knowledge Graph","Event Causality Identification"],"title":"ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification","type":"publication"},{"authors":["Haonan Qiu","Siyu Chen","Bei Gan","Kun Wang","Huafeng Shi","Jing Shao","Ziwei Liu"],"categories":[],"content":"","date":1649721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"dd13be2565b1abd5266003d41284989b","permalink":"https://amandajshao.github.io/publication/qiu-few-shot-2022/","publishdate":"2022-08-26T15:26:32.66179Z","relpermalink":"/publication/qiu-few-shot-2022/","section":"publication","summary":"Realistic visual media synthesis is becoming a critical societal issue with the surge of face manipulation models; new forgery approaches emerge at an unprecedented pace. Unfortunately, existing forgery detection methods suffer significant performance drops when applied to novel forgery approaches. In this work, we address the few-shot forgery detection problem by designing a comprehensive benchmark based on coverage analysis among various forgery approaches, and proposing Guided Adversarial Interpolation (GAI). Our key insight is that there exist transferable distribution characteristics among different forgery approaches with the majority and minority classes. Specifically, we enhance the discriminative ability against novel forgery approaches via adversarially interpolating the artifacts of the minority samples to the majority samples under the guidance of a teacher network. Unlike the standard re-balancing method which usually results in over-fitting to minority classes, our method simultaneously takes account of the diversity of majority information as well as the significance of minority information. Extensive experiments demonstrate that our GAI achieves state-of-the-art performances on the established few-shot forgery detection benchmark. Notably, our method is also validated to be robust to choices of majority and minority forgery approaches.","tags":[],"title":"Few-shot Forgery Detection via Guided Adversarial Interpolation","type":"publication"},{"authors":["Yufeng Cui","Lichen Zhao","Feng Liang","Yangguang Li","Jing Shao"],"categories":["General Representation Learning"],"content":"","date":1646956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"cf0a7f27d90124329ce67c10e8937b20","permalink":"https://amandajshao.github.io/publication/cui-democratizing-2022/","publishdate":"2022-08-26T15:26:32.275163Z","relpermalink":"/publication/cui-democratizing-2022/","section":"publication","summary":"","tags":["Vision and Language","Pre-training","Contrastive Learning","CLIP","Benchmark"],"title":"Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision","type":"publication"},{"authors":["Yangguang Li","Feng Liang","Lichen Zhao","Yufeng Cui","Wanli Ouyang","Jing Shao","Fengwei Yu","Junjie Yan"],"categories":[],"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"19c1874a6ea0ca19d56705b112bfa373","permalink":"https://amandajshao.github.io/publication/li-supervision-2022/","publishdate":"2022-08-26T15:26:29.71034Z","relpermalink":"/publication/li-supervision-2022/","section":"publication","summary":"Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to...","tags":[],"title":"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm","type":"publication"},{"authors":["Yuanhan Zhang","Yichao Wu","Zhenfei Yin","Jing Shao","Ziwei Liu"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"49f1866c1e27074920ac4b54a8b0a909","permalink":"https://amandajshao.github.io/publication/zhang-robust-2022/","publishdate":"2022-08-26T15:26:32.913968Z","relpermalink":"/publication/zhang-robust-2022/","section":"publication","summary":"","tags":[],"title":"Robust Face Anti-Spoofing with Dual Probabilistic Modeling","type":"publication"},{"authors":["Hao Wang","Yangguang Li","Zhen Huang","Yong Dou","Lingpeng Kong","Jing Shao"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"291d13681dc75dd55ee818b97c674c5a","permalink":"https://amandajshao.github.io/publication/wang-sncse-2022/","publishdate":"2022-08-26T15:26:32.147539Z","relpermalink":"/publication/wang-sncse-2022/","section":"publication","summary":"","tags":[],"title":"SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples","type":"publication"},{"authors":["Junting Pan","Ziyi Lin","Xiatian Zhu","Jing Shao","Hongsheng Li"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"2b5b5bff0ee74611538fcd98fac98d5f","permalink":"https://amandajshao.github.io/publication/pan-st-adapter-2022/","publishdate":"2022-08-26T15:26:33.167369Z","relpermalink":"/publication/pan-st-adapter-2022/","section":"publication","summary":"","tags":[],"title":"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition","type":"publication"},{"authors":["Ruining Tang","Zhenyu Liu","Yangguang Li","Yiguo Song","Hui Liu","Qide Wang","Jing Shao","Guifang Duan","Jianrong Tan"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"45c50b9ac2b9e5e9f52a9a66cd02e143","permalink":"https://amandajshao.github.io/publication/tang-task-balanced-2022/","publishdate":"2022-08-26T15:26:33.419104Z","relpermalink":"/publication/tang-task-balanced-2022/","section":"publication","summary":"","tags":[],"title":"Task-Balanced Distillation for Object Detection","type":"publication"},{"authors":["Yinan He","Lu Sheng","Jing Shao","Ziwei Liu","Zhaofan Zou","Zhizhi Guo","Shan Jiang","Curitis Sun","Guosheng Zhang","Keyao Wang","Haixiao Yue","Zhibin Hong","Wanguo Wang","Zhenyu Li","Qi Wang","Zhenli Wang","Ronghao Xu","Mingwen Zhang","Zhiheng Wang","Zhenhang Huang","Tianming Zhang","Ningning Zhao"],"categories":[],"content":"","date":1639526400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527594,"objectID":"eee45269796e16f55546e07fc774c449","permalink":"https://amandajshao.github.io/publication/he-forgerynet-2021-1/","publishdate":"2022-08-26T15:26:33.92416Z","relpermalink":"/publication/he-forgerynet-2021-1/","section":"publication","summary":"","tags":[],"title":"ForgeryNet - Face Forgery Analysis Challenge 2021: Methods and Results","type":"publication"},{"authors":["Teli Ma","Shijie Geng","Mengmeng Wang","Jing Shao","Jiasen Lu","Hongsheng Li","Peng Gao","Yu Qiao"],"categories":[],"content":"","date":1638144e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"28634091258ef5641a544c7c98e54427","permalink":"https://amandajshao.github.io/publication/ma-simple-2021/","publishdate":"2022-08-26T15:26:33.798275Z","relpermalink":"/publication/ma-simple-2021/","section":"publication","summary":"","tags":[],"title":"A Simple Long-Tailed Recognition Baseline via Vision-Language Model","type":"publication"},{"authors":["Jing Shao","Siyu Chen","Yangguang Li","Kun Wang","Zhenfei Yin","Yinan He","Jianing Teng","Qinghong Sun","Mengya Gao","Jihao Liu","Gengshi Huang","Guanglu Song","Yichao Wu","Yuming Huang","Fenggang Liu","Huan Peng","Shuo Qin","Chengyu Wang","Yujie Wang","Conghui He","Ding Liang","Yu Liu","Fengwei Yu","Junjie Yan","Dahua Lin","Xiaogang Wang","Yu Qiao"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"941d33d7b90d790a0483e94902024c90","permalink":"https://amandajshao.github.io/publication/shao-intern-2021/","publishdate":"2022-08-26T15:26:33.674136Z","relpermalink":"/publication/shao-intern-2021/","section":"publication","summary":"Enormous waves of technological innovations over the past several years, marked by the advances in AI technologies, are profoundly reshaping the industry and the society. However, down the road, a key challenge awaits us, that is, our capability of meeting rapidly-growing scenario-specific demands is severely limited by the cost of acquiring a commensurate amount of training data. This difficult situation is in essence due to limitations of the mainstream learning paradigm: we need to train a new model for each new scenario, based on a large quantity of well-annotated data and commonly from scratch. In tackling this fundamental problem, we move beyond and develop a new learning paradigm named INTERN. By learning with supervisory signals from multiple sources in multiple stages, the model being trained will develop strong generalizability. We evaluate our model on 26 well-known datasets that cover four categories of tasks in computer vision. In most cases, our models, adapted with only 10% of the training data in the target domain, outperform the counterparts trained with the full set of data, often by a significant margin. This is an important step towards a promising prospect where such a model with general vision capability can dramatically reduce our reliance on data, thus expediting the adoption of AI technologies. Furthermore, revolving around our new paradigm, we also introduce a new data system, a new architecture, and a new benchmark, which, together, form a general vision ecosystem to support its future development in an open and inclusive manner. See project website at https://opengvlab.shlab.org.cn .","tags":[],"title":"INTERN: A New Learning Paradigm Towards General Vision","type":"publication"},{"authors":["Zhao Zhong","Zichen Yang","Boyang Deng","Junjie Yan","Wei Wu","Jing Shao","Cheng-Lin Liu"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"67d31f4d0af1640a6c4f2e2b76296f8e","permalink":"https://amandajshao.github.io/publication/zhong-blockqnn-2021/","publishdate":"2022-08-26T15:26:29.961058Z","relpermalink":"/publication/zhong-blockqnn-2021/","section":"publication","summary":"Convolutional neural networks have gained a remarkable success in computer vision. However, most popular network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35 percent top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0 percent top-1 and 96.0 percent top-5 on ImageNet.","tags":["Acceleration","AutoML","Computer architecture","Convolutional neural network","Graphics processing units","Indexes","Network architecture","neural architecture search","Neural networks","Q-learning","reinforcement learning","Task analysis"],"title":"BlockQNN: Efficient Block-Wise Neural Network Architecture Generation","type":"publication"},{"authors":["Junting Pan","Siyu Chen","Mike Zheng Shou","Yu Liu","Jing Shao","Hongsheng Li"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527594,"objectID":"05f6a7588ae4be66d709d574eb92b3ce","permalink":"https://amandajshao.github.io/publication/pan-actor-context-actor-2021/","publishdate":"2022-08-26T15:26:34.431776Z","relpermalink":"/publication/pan-actor-context-actor-2021/","section":"publication","summary":"","tags":[],"title":"Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization","type":"publication"},{"authors":["Yinan He","Bei Gan","Siyu Chen","Yichun Zhou","Guojun Yin","Luchuan Song","Lu Sheng","Jing Shao","Ziwei Liu"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527587,"objectID":"3dd31546731053f09ad0256b0afcd0b5","permalink":"https://amandajshao.github.io/publication/he-forgerynet-2021/","publishdate":"2022-08-26T15:26:27.608609Z","relpermalink":"/publication/he-forgerynet-2021/","section":"publication","summary":"The rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis.To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real/fake), three-way (real/fake with identity-replaced forgery approaches/fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations. We hope that the scale, quality, and variety of our ForgeryNet dataset will foster further research and innovation in the area of face forgery classification, as well as spatial and temporal forgery localization etc.","tags":[],"title":"ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis","type":"publication"},{"authors":["Bowen Yang","Jing Zhang","Zhenfei Yin","Jing Shao"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527593,"objectID":"782d73369983a6ba3fe32b993e943c07","permalink":"https://amandajshao.github.io/publication/yang-few-shot-2021/","publishdate":"2022-08-26T15:26:33.546206Z","relpermalink":"/publication/yang-few-shot-2021/","section":"publication","summary":"","tags":[],"title":"Few-Shot Domain Expansion for Face Anti-Spoofing","type":"publication"},{"authors":["Yuanhan Zhang","ZhenFei Yin","Yidong Li","Guojun Yin","Junjie Yan","Jing Shao","Ziwei Liu"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"ca1b97bc373fa0e2c2b4a59f0206a9d9","permalink":"https://amandajshao.github.io/publication/vedaldi-celeba-spoof-2020/","publishdate":"2022-08-26T15:26:30.091318Z","relpermalink":"/publication/vedaldi-celeba-spoof-2020/","section":"publication","summary":"**CelebA-Spoof** is a large-scale face anti-spoofing dataset that has 625,537 images from 10,177 subjects, which includes 43 rich attributes on face, illumination, environment and spoof types. Live images are selected from the CelebA dataset. We collect and annotate spoof images for CelebA-Spoof. Among 43 rich attributes, 40 attributes belong to live images including all facial components and accessories such as skin, nose, eyes, eyebrows, lip, hair, hat, eyeglass. 3 attributes belong to spoof images including spoof types, environments and illumination conditions. **CelebA-Spoof** can be used to train and evaluate algorithms of face anti-spoofing, face presentation attacks, and robustness/security research.","tags":[],"title":"CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations","type":"publication"},{"authors":["Kun Yuan","Quanquan Li","Jing Shao","Junjie Yan"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"d77431a240b4bfa4c0bbfee7feff1a68","permalink":"https://amandajshao.github.io/publication/vedaldi-learning-2020/","publishdate":"2022-08-26T15:26:30.215272Z","relpermalink":"/publication/vedaldi-learning-2020/","section":"publication","summary":"Seeking eﬀective neural networks is a critical and practical ﬁeld in deep learning. Besides designing the depth, type of convolution, normalization, and nonlinearities, the topological connectivity of neural networks is also important. Previous principles of rule-based modular design simplify the diﬃculty of building an eﬀective architecture, but constrain the possible topologies in limited spaces. In this paper, we attempt to optimize the connectivity in neural networks. We propose a topological perspective to represent a network into a complete graph for analysis, where nodes carry out aggregation and transformation of features, and edges determine the ﬂow of information. By assigning learnable parameters to the edges which reﬂect the magnitude of connections, the learning process can be performed in a diﬀerentiable manner. We further attach auxiliary sparsity constraint to the distribution of connectedness, which promotes the learned topology focus on critical connections. This learning process is compatible with existing networks and owns adaptability to larger search spaces and diﬀerent tasks. Quantitative results of experiments reﬂect the learned connectivity is superior to traditional rule-based ones, such as random, residual and complete. In addition, it obtains signiﬁcant improvements in image classiﬁcation and object detection without introducing excessive computation burden.","tags":[],"title":"Learning Connectivity of Neural Networks from a Topological Perspective","type":"publication"},{"authors":["Ronghao Guo","Chen Lin","Chuming Li","Keyu Tian","Ming Sun","Lu Sheng","Junjie Yan"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"1e135fb7a08d2b9a5567da93b99cb1eb","permalink":"https://amandajshao.github.io/publication/vedaldi-powering-2020/","publishdate":"2022-08-26T15:26:27.918192Z","relpermalink":"/publication/vedaldi-powering-2020/","section":"publication","summary":"One-shot NAS method has attracted much interest from the research community due to its remarkable training eﬃciency and capacity to discover high performance models. However, the search spaces of previous one-shot based works usually relied on hand-craft design and were short for ﬂexibility on the network topology. In this work, we try to enhance the one-shot NAS by exploring high-performing network architectures in our large-scale Topology Augmented Search Space (i.e, over 3.4 × 1010 diﬀerent topological structures). Speciﬁcally, the diﬃculties for architecture searching in such a complex space has been eliminated by the proposed stabilized share-parameter proxy, which employs Stochastic Gradient Langevin Dynamics to enable fast shared parameter sampling, so as to achieve stabilized measurement of architecture performance even in search space with complex topological structures. The proposed method, namely Stablized Topological Neural Architecture Search (ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds) constraint on ImageNet. Our lite model ST-NAS-A achieves 76.4% top-1 accuracy with only 326M MAdds. Our moderate model STNAS-B achieves 77.9% top-1 accuracy just required 503M MAdds. Both of our models oﬀer superior performances in comparison to other concurrent works on one-shot NAS.","tags":[],"title":"Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy","type":"publication"},{"authors":["Yuyang Qian","Guojun Yin","Lu Sheng","Zixuan Chen","Jing Shao"],"categories":[],"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527587,"objectID":"819af01444060ebded6d7406b4472521","permalink":"https://amandajshao.github.io/publication/vedaldi-thinking-2020/","publishdate":"2022-08-26T15:26:27.79423Z","relpermalink":"/publication/vedaldi-thinking-2020/","section":"publication","summary":"As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We ﬁnd that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two diﬀerent but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net signiﬁcantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.","tags":[],"title":"Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues","type":"publication"},{"authors":["Siyu Chen","Junting Pan","Guanglu Song","Manyuan Zhang","Hao Shao","Ziyi Lin","Jing Shao","Hongsheng Li","Yu Liu"],"categories":["Image and Video Understanding"],"content":"","date":1592265600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527594,"objectID":"c4564c37affd7b8bdf2f89dc848b9a78","permalink":"https://amandajshao.github.io/publication/chen-1-st-2020/","publishdate":"2022-08-26T15:26:34.052152Z","relpermalink":"/publication/chen-1-st-2020/","section":"publication","summary":"","tags":["Action Recognition","AcitivityNet Challenge"],"title":"1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020","type":"publication"},{"authors":["Lu Sheng","Junting Pan","Jiaming Guo","Jing Shao","Chen Change Loy"],"categories":[],"content":"","date":1590624e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"d89ff34a31319e3f2164d5c3da9129b0","permalink":"https://amandajshao.github.io/publication/sheng-high-quality-2020/","publishdate":"2022-08-26T15:26:28.047791Z","relpermalink":"/publication/sheng-high-quality-2020/","section":"publication","summary":"This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released: https://github.com/junting/seg2vid.","tags":["Conditioned generative model","Image and video synthesis","Motion prediction and estimatiovn","Unsupervised learning"],"title":"High-Quality Video Generation from Static Structural Annotations","type":"publication"},{"authors":["Minghua Liu","Lu Sheng","Sheng Yang","Jing Shao","Shi-Min Hu"],"categories":[],"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"33d944588f85e69c4c01298886bea42c","permalink":"https://amandajshao.github.io/publication/liu-morphing-2020/","publishdate":"2022-08-26T15:26:28.174916Z","relpermalink":"/publication/liu-morphing-2020/","section":"publication","summary":"3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention in the community. For acquiring high-fidelity dense point clouds and avoiding uneven distribution, blurred details, or structural loss of existing methods' results, we propose a novel approach to complete the partial point cloud in two stages. Specifically, in the first stage, the approach predicts a complete but coarse-grained point cloud with a collection of parametric surface elements. Then, in the second stage, it merges the coarse-grained prediction with the input point cloud by a novel sampling algorithm. Our method utilizes a joint loss function to guide the distribution of the points. Extensive experiments verify the effectiveness of our method and demonstrate that it outperforms the existing methods in both the Earth Mover's Distance (EMD) and the Chamfer Distance (CD).","tags":[],"title":"Morphing and Sampling Network for Dense Point Cloud Completion","type":"publication"},{"authors":["Zihao Wang","Chen Lin","Lu Sheng","Junjie Yan","Jing Shao"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527594,"objectID":"2c5b27b5f81d1fbb05d7b3d063fcaa31","permalink":"https://amandajshao.github.io/publication/wang-pv-nas-2020/","publishdate":"2022-08-26T15:26:34.175823Z","relpermalink":"/publication/wang-pv-nas-2020/","section":"publication","summary":"","tags":[],"title":"PV-NAS: Practical Neural Architecture Search for Video Recognition","type":"publication"},{"authors":["Xihui Liu","Guojun Yin","Jing Shao","Xiaogang Wang","hongsheng Li"],"categories":[],"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"134b2548a285c56ac234f0f2a8d92d67","permalink":"https://amandajshao.github.io/publication/liu-learning-2019/","publishdate":"2022-08-26T15:26:30.4742Z","relpermalink":"/publication/liu-learning-2019/","section":"publication","summary":"Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.","tags":[],"title":"Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis","type":"publication"},{"authors":["Zihao Wang","Xihui Liu","Hongsheng Li","Lu Sheng","Junjie Yan","Xiaogang Wang","Jing Shao"],"categories":[],"content":"","date":1569888e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"f55e66ed1ee4dd0d44614fb53f0e9946","permalink":"https://amandajshao.github.io/publication/wang-camp-2019/","publishdate":"2022-08-26T15:26:28.298788Z","relpermalink":"/publication/wang-camp-2019/","section":"publication","summary":"Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.","tags":[],"title":"CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval","type":"publication"},{"authors":["Guojun Yin","Lu Sheng","Bin Liu","Nenghai Yu","Xiaogang Wang","Jing Shao"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"586eb15f2c33a2ee8a24a9976bb995f0","permalink":"https://amandajshao.github.io/publication/yin-context-2019/","publishdate":"2022-08-26T15:26:28.558183Z","relpermalink":"/publication/yin-context-2019/","section":"publication","summary":"Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.","tags":[],"title":"Context and Attribute Grounded Dense Captioning","type":"publication"},{"authors":["Xihui Liu","Zihao Wang","Jing Shao","Xiaogang Wang","Hongsheng Li"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"cf6615aa67e3c7a3659fb3864a745135","permalink":"https://amandajshao.github.io/publication/liu-improving-2019/","publishdate":"2022-08-26T15:26:30.344414Z","relpermalink":"/publication/liu-improving-2019/","section":"publication","summary":"Referring expression grounding aims at locating certain objects or persons in an image with a referring expression, where the key challenge is to comprehend and align various types of information from visual and textual domain, such as visual attributes, location and interactions with surrounding regions. Although the attention mechanism has been successfully applied for cross-modal alignments, previous attention models focus on only the most dominant features of both modalities, and neglect the fact that there could be multiple comprehensive textual-visual correspondences between images and referring expressions. To tackle this issue, we design a novel cross-modal attention-guided erasing approach, where we discard the most dominant information from either textual or visual domains to generate difﬁcult training samples online, and to drive the model to discover complementary textual-visual correspondences. Extensive experiments demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance on three referring expression grounding datasets.","tags":[],"title":"Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing","type":"publication"},{"authors":["Guojun Yin","Bin Liu","Lu Sheng","Nenghai Yu","Xiaogang Wang","Jing Shao"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"46bc0d9abda1e17147c88ab5cf94a89f","permalink":"https://amandajshao.github.io/publication/yin-semantics-2019/","publishdate":"2022-08-26T15:26:29.198881Z","relpermalink":"/publication/yin-semantics-2019/","section":"publication","summary":"Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.","tags":["Image and Video Synthesis","Vision + Language"],"title":"Semantics Disentangling for Text-To-Image Generation","type":"publication"},{"authors":["Junting Pan","Chengyu Wang","Xu Jia","Jing Shao","Lu Sheng","Junjie Yan","Xiaogang Wang"],"categories":[],"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"199c5ba9f4e413d484de33706ab602e5","permalink":"https://amandajshao.github.io/publication/pan-video-2019/","publishdate":"2022-08-26T15:26:28.426957Z","relpermalink":"/publication/pan-video-2019/","section":"publication","summary":"This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between ﬂexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difﬁcult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the ﬁrst frame. Then we animate the scene based on its semantic meaning to obtain temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical ﬂow as a beneﬁcial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the ﬂow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods. The source code will be released on https://github.com/junting/seg2vid.","tags":[],"title":"Video Generation From Single Semantic Label Map","type":"publication"},{"authors":["Lu Sheng","Junting Pan","Jiaming Guo","Jing Shao","Xiaogang Wang","Chen Change Loy"],"categories":[],"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527594,"objectID":"48d7a79e3c56960d8bed32b74e4f5760","permalink":"https://amandajshao.github.io/publication/sheng-unsupervised-2019/","publishdate":"2022-08-26T15:26:34.302935Z","relpermalink":"/publication/sheng-unsupervised-2019/","section":"publication","summary":"","tags":[],"title":"Unsupervised Bi-directional Flow-based Video Generation from one Snapshot","type":"publication"},{"authors":["Yongcheng Liu","Lu Sheng","Jing Shao","Junjie Yan","Shiming Xiang","Chunhong Pan"],"categories":[],"content":"","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"c6c5740d304e4f2951793caeb5e8b3cc","permalink":"https://amandajshao.github.io/publication/liu-multi-label-2018/","publishdate":"2022-08-26T15:26:28.689518Z","relpermalink":"/publication/liu-multi-label-2018/","section":"publication","summary":"Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.","tags":["knowledge distillation","multi-label image classification","weakly-supervised detection"],"title":"Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection","type":"publication"},{"authors":["Xihui Liu","Hongsheng Li","Jing Shao","Dapeng Chen","Xiaogang Wang"],"categories":["Vision and Language"],"content":"","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"5c26a63cda6ed64351d2a304ba23cf9c","permalink":"https://amandajshao.github.io/publication/ferrari-show-2018/","publishdate":"2022-08-26T15:26:30.989325Z","relpermalink":"/publication/ferrari-show-2018/","section":"publication","summary":"The aim of image captioning is to generate captions by machine to describe image contents. Despite many eﬀorts, generating discriminative captions for images remains non-trivial. Most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. In this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. It brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the quality of generated captions. (2) The correspondence between generated captions and images are naturally incorporated in the generation process without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional annotations. We demonstrate the eﬀectiveness of the proposed retrievalguided method on COCO and Flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions.","tags":["Image Captioning","Partially Labelled Data"],"title":"Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data","type":"publication"},{"authors":["Yu Liu","Guanglu Song","Jing Shao","Xiao Jin","Xiaogang Wang"],"categories":[],"content":"","date":1536192e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"6489d7d82f88bd17380ae2b3ef5c7681","permalink":"https://amandajshao.github.io/publication/ferrari-transductive-2018/","publishdate":"2022-08-26T15:26:30.859591Z","relpermalink":"/publication/ferrari-transductive-2018/","section":"publication","summary":"Conventional deep semi-supervised learning methods, such as recursive clustering and training process, suﬀer from cumulative error and high computational complexity when collaborating with Convolutional Neural Networks. To this end, we design a simple but eﬀective learning mechanism that merely substitutes the last fully-connected layer with the proposed Transductive Centroid Projection (TCP) module. It is inspired by the observation of the weights in the ﬁnal classiﬁcation layer (called anchors) converge to the central direction of each class in hyperspace. Speciﬁcally, we design the TCP module by dynamically adding an ad hoc anchor for each cluster in one mini-batch. It essentially reduces the probability of the inter-class conﬂict and enables the unlabelled data functioning as labelled data. We inspect its eﬀectiveness with elaborate ablation study on seven public face/person classiﬁcation benchmarks. Without any bells and whistles, TCP can achieve signiﬁcant performance gains over most state-of-the-art methods in both fully-supervised and semi-supervised manners.","tags":[],"title":"Transductive Centroid Projection for Semi-supervised Large-Scale Recognition","type":"publication"},{"authors":["Pengze Liu","Xihui Liu","Junjie Yan","Jing Shao"],"categories":[],"content":"","date":153576e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"8ecd6c077f3c19a888156a29063cc4a8","permalink":"https://amandajshao.github.io/publication/liu-localization-2018/","publishdate":"2022-08-26T15:26:30.603238Z","relpermalink":"/publication/liu-localization-2018/","section":"publication","summary":"","tags":[],"title":"Localization Guided Learning for Pedestrian Attribute Recognition","type":"publication"},{"authors":["Lu Sheng","Ziyi Lin","Jing Shao","Xiaogang Wang"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"64a7b4a82b4ba40bf4002e7e81bee3fb","permalink":"https://amandajshao.github.io/publication/sheng-avatar-net-2018/","publishdate":"2022-08-26T15:26:29.073693Z","relpermalink":"/publication/sheng-avatar-net-2018/","section":"publication","summary":"Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efﬁciency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efﬁcient yet effective Avatar-Net that enables visually plausible multiscale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multiscale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efﬁciency of the proposed method in generating high-quality stylized images, with a series of successive applications include multiple style integration, video stylization and etc.","tags":[],"title":"Avatar-Net: Multi-scale Zero-Shot Style Transfer by Feature Decoration","type":"publication"},{"authors":["Yu Liu","Fangyin Wei","Jing Shao","Lu Sheng","Junjie Yan","Xiaogang Wang"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"97a6fdf77c4463b9fcbafccdee1fb06a","permalink":"https://amandajshao.github.io/publication/liu-exploring-2018/","publishdate":"2022-08-26T15:26:28.946822Z","relpermalink":"/publication/liu-exploring-2018/","section":"publication","summary":"This paper proposes learning disentangled but complementary face features with a minimal supervision by face identiﬁcation. Speciﬁcally, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity veriﬁcation and the identity-dispelled features to fool the veriﬁcation system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity veriﬁcation performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.","tags":[],"title":"Exploring Disentangled Feature Representation Beyond Face Identification","type":"publication"},{"authors":["Zhao Zhong","Junjie Yan","Wei Wu","Jing Shao","Cheng-Lin Liu"],"categories":[],"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527590,"objectID":"67313322de95f44a56513283d1a9cc46","permalink":"https://amandajshao.github.io/publication/zhong-practical-2018/","publishdate":"2022-08-26T15:26:30.728728Z","relpermalink":"/publication/zhong-practical-2018/","section":"publication","summary":"Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classiﬁcation, additionally, the best network generated by BlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.","tags":[],"title":"Practical Block-Wise Neural Network Architecture Generation","type":"publication"},{"authors":["Dapeng Chen","Hongsheng Li","Xihui Liu","Yantao Shen","Jing Shao","Zejian Yuan","Xiaogang Wang"],"categories":["Visual Surveillance"],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"3cc628f65d18a12e9bf5317e18f03fb9","permalink":"https://amandajshao.github.io/publication/ferrari-improving-2018/","publishdate":"2022-08-26T15:26:31.244365Z","relpermalink":"/publication/ferrari-improving-2018/","section":"publication","summary":"Person re-identiﬁcation is an important task that requires learning discriminative visual features for distinguishing diﬀerent person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for eﬀective visual features. Compared with other auxiliary information, language can describe a speciﬁc person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the eﬀectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.","tags":["Person Re-identification","Vision and Language"],"title":"Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association","type":"publication"},{"authors":["Guojun Yin","Lu Sheng","Bin Liu","Nenghai Yu","Xiaogang Wang","Jing Shao","Chen Change Loy"],"categories":[],"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527588,"objectID":"a5243ef6d3ce5c7d2335d622fcb040e9","permalink":"https://amandajshao.github.io/publication/ferrari-zoom-net-2018/","publishdate":"2022-08-26T15:26:28.815336Z","relpermalink":"/publication/ferrari-zoom-net-2018/","section":"publication","summary":"Recognizing visual relationships subject-predicate-object among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our ﬁnal Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the eﬀectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42% from 8.16%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.","tags":[],"title":"Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition","type":"publication"},{"authors":["Xihui Liu","Haiyu Zhao","Maoqing Tian","Lu Sheng","Jing Shao","Shuai Yi","Junjie Yan","Xiaogang Wang"],"categories":[],"content":"","date":1506816e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527589,"objectID":"843e6d00ae1011c75bef4f4720bfdf88","permalink":"https://amandajshao.github.io/publication/liu-hydraplus-net-2017/","publishdate":"2022-08-26T15:26:29.323878Z","relpermalink":"/publication/liu-hydraplus-net-2017/","section":"publication","summary":"Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attentionbased deep neural network, named as HydraPlus-Net (HPnet), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person reidentification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-theart methods on various datasets.","tags":["Pedestrian Analysis"],"title":"HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis","type":"publication"},{"authors":["Zhongdao Wang","Luming Tang","Xihui Liu","Zhuliang Yao","Shuai Yi","Jing Shao","Junjie Yan","Shengjin Wang","Hongsheng Li","Xiaogang Wang"],"categories":[],"content":"","date":1506816e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"500a4329229c22951bc8151b29801c27","permalink":"https://amandajshao.github.io/publication/wang-orientation-2017/","publishdate":"2022-08-26T15:26:31.629976Z","relpermalink":"/publication/wang-orientation-2017/","section":"publication","summary":"In this paper, we tackle the vehicle Re-identification (ReID) problem which is of great importance in urban surveillance and can be used for multiple applications. In our vehicle ReID framework, an orientation invariant feature embedding module and a spatial-temporal regularization module are proposed. With orientation invariant feature embedding, local region features of different orientations can be extracted based on 20 key point locations and can be well aligned and combined. With spatial-temporal regularization, the log-normal distribution is adopted to model the spatial-temporal constraints and the retrieval results can be refined. Experiments are conducted on public vehicle ReID datasets and our proposed method achieves state-of-the-art performance. Investigations of the proposed framework is conducted, including the landmark regressor and comparisons with attention mechanism. Both the orientation invariant feature embedding and the spatio-temporal regularization achieve considerable improvements.","tags":["Automobiles","Feature extraction","Pipelines","Proposals","Surveillance","Wheels"],"title":"Orientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-identification","type":"publication"},{"authors":["Haiyu Zhao","Maoqing Tian","Shuyang Sun","Jing Shao","Junjie Yan","Shuai Yi","Xiaogang Wang","Xiaoou Tang"],"categories":[],"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"b7ca26f5705da19ca2ad11a74fb105c5","permalink":"https://amandajshao.github.io/publication/zhao-spindle-2017/","publishdate":"2022-08-26T15:26:31.503252Z","relpermalink":"/publication/zhao-spindle-2017/","section":"publication","summary":"","tags":[],"title":"Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion","type":"publication"},{"authors":["Jing Shao","Chen Change Loy","Xiaogang Wang"],"categories":[],"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"3f46449799e080ae966305a23ecbca39","permalink":"https://amandajshao.github.io/publication/shao-learning-2017/","publishdate":"2022-08-26T15:26:31.12069Z","relpermalink":"/publication/shao-learning-2017/","section":"publication","summary":"Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this paper, we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel collective transition prior, which leads to a robust approach for group segregation in public spaces. From the former, we further devise a rich set of group-property visual descriptors. These descriptors are scene-independent and can be effectively applied to public scenes with a variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are complementary to each other, scene-independent, and they convey critical information on physical states of a crowd. The proposed group-level descriptors show promising results and potentials in multiple applications, including crowd dynamic monitoring, crowd video classification, and crowd video retrieval.","tags":["Circuit stability","Crowded scene understanding","Feature extraction","group-property analysis","Hidden Markov models","Psychology","Robustness","Stability analysis","video analysis","Visualization"],"title":"Learning Scene-Independent Group Descriptors for Crowd Understanding","type":"publication"},{"authors":["Jing Shao","Chen Change Loy","Kai Kang","Xiaogang Wang"],"categories":[],"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"687dfd4e1d56023fdd052449696b06e1","permalink":"https://amandajshao.github.io/publication/shao-crowded-2017/","publishdate":"2022-08-26T15:26:31.372114Z","relpermalink":"/publication/shao-crowded-2017/","section":"publication","summary":"Crowd video analysis is one of the hallmark tasks of crowded scene understanding. While we observe a tremendous progress in image-based tasks with the rise of convolutional neural networks (CNNs), performance on video analysis has not (yet) attained the same level of success. In this paper, we introduce intuitive but effective temporal-aware crowd motion channels by uniformly slicing the video volume from different dimensions. Multiple CNN structures with different data-fusion strategies and weight-sharing schemes are proposed to learn the connectivity both spatially and temporally from these motion channels. To well demonstrate our deep model, we construct a new large-scale Who do What at someWhere crowd data set with 10 000 videos from 8257 crowded scenes, and build an attribute set with 94 attributes. Extensive experiments on crowd video attribute prediction demonstrate the effectiveness of our novel method over the state-of-the-art.","tags":["Crowd database","crowded scene understanding","deep neural network","Feature extraction","Image segmentation","Motion segmentation","Neural networks","Semantics","spatiotemporal features","Surveillance","video analysis","World Wide Web"],"title":"Crowded Scene Understanding by Deeply Learned Volumetric Slices","type":"publication"},{"authors":["Jing Shao","Chen Change Loy","Kai Kang","Xiaogang Wang"],"categories":[],"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527591,"objectID":"1e1d38038afa18a0f32d810c6231ec47","permalink":"https://amandajshao.github.io/publication/shao-slicing-2016/","publishdate":"2022-08-26T15:26:31.758117Z","relpermalink":"/publication/shao-slicing-2016/","section":"publication","summary":"Learning and capturing both appearance and dynamic representations are pivotal for crowd video understanding. Convolutional Neural Networks (CNNs) have shown its remarkable potential in learning appearance representations from images. However, the learning of dynamic representation, and how it can be effectively combined with appearance features for video analysis, remains an open problem. In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatio-and 2D temporal-slices representations. The decomposition brings unique advantages: (1) the model is capable of capturing dynamics of different semantic units such as groups and objects, (2) it learns separated appearance and dynamic representations while keeping proper interactions between them, and (3) it exploits the selectiveness of spatial filters to discard irrelevant background clutter for crowd understanding. We demonstrate the effectiveness of the proposed S-CNN model on the WWW crowd video dataset for attribute recognition and observe significant performance improvements to the state-of-the-art methods (62.55% from 51.84% [21]).","tags":["Dynamics","Feature extraction","Ice","Semantics","Three-dimensional displays","Two dimensional displays","Visualization"],"title":"Slicing Convolutional Neural Network for Crowd Video Understanding","type":"publication"},{"authors":["Jing Shao","Kai Kang","Chen Change Loy","Xiaogang Wang"],"categories":[],"content":"","date":1433116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"8e2e58f87647dddcd209e0e638c52a2e","permalink":"https://amandajshao.github.io/publication/shao-deeply-2015/","publishdate":"2022-08-26T15:26:32.012643Z","relpermalink":"/publication/shao-deeply-2015/","section":"publication","summary":"Crowded scene understanding is a fundamental problem in computer vision. In this study, we develop a multi-task deep model to jointly learn and combine appearance and motion features for crowd understanding. We propose crowd motion channels as the input of the deep model and the channel design is inspired by generic properties of crowd systems. To well demonstrate our deep model, we construct a new large-scale WWW Crowd dataset with 10, 000 videos from 8, 257 crowded scenes, and build an attribute set with 94 attributes on WWW. We further measure user study performance on WWW and compare this with the proposed deep models. Extensive experiments show that our deep models display significant performance improvements in cross-scene attribute recognition compared to strong crowd-related feature-based baselines, and the deeply learned features behave a superior performance in multi-task learning.","tags":["Accuracy","Computational modeling","Stability analysis","Time factors","Tracking","Videos","World Wide Web"],"title":"Deeply learned attributes for crowded scene understanding","type":"publication"},{"authors":["Jing Shao","Chen Change Loy","Xiaogang Wang"],"categories":[],"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661527592,"objectID":"c7998b01b1334e39654694ddd2008e4b","permalink":"https://amandajshao.github.io/publication/shao-scene-independent-2014/","publishdate":"2022-08-26T15:26:31.885537Z","relpermalink":"/publication/shao-scene-independent-2014/","section":"publication","summary":"Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this study we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel Collective Transition prior, which leads to a robust approach for group segregation in public spaces. From the prior, we further devise a rich set of group property visual descriptors. These descriptors are scene-independent, and can be effectively applied to public-scene with variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are not only useful but also necessary for group state analysis and crowd scene understanding.","tags":["Computed tomography","Context","Robustness","Stability criteria","Tracking","Visualization"],"title":"Scene-Independent Group Profiling in Crowd","type":"publication"}]