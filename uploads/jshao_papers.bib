
@inproceedings{he_forgerynet_2021,
	address = {Nashville, TN, USA},
	title = {{ForgeryNet}: {A} {Versatile} {Benchmark} for {Comprehensive} {Forgery} {Analysis}},
	isbn = {978-1-66544-509-2},
	shorttitle = {{ForgeryNet}},
	url = {https://ieeexplore.ieee.org/document/9577546/},
	doi = {10.1109/CVPR46437.2021.00434},
	abstract = {The rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Yinan and Gan, Bei and Chen, Siyu and Zhou, Yichun and Yin, Guojun and Song, Luchuan and Sheng, Lu and Shao, Jing and Liu, Ziwei},
	month = jun,
	year = {2021},
	pages = {4358--4367},
	file = {He et al. - 2021 - ForgeryNet A Versatile Benchmark for Comprehensiv.pdf:/Users/lucasjing/Zotero/storage/R3VICK3Q/He et al. - 2021 - ForgeryNet A Versatile Benchmark for Comprehensiv.pdf:application/pdf},
}

@incollection{vedaldi_thinking_2020,
	address = {Cham},
	title = {Thinking in {Frequency}: {Face} {Forgery} {Detection} by {Mining} {Frequency}-{Aware} {Clues}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	shorttitle = {Thinking in {Frequency}},
	url = {https://link.springer.com/10.1007/978-3-030-58610-2_6},
	abstract = {As realistic facial manipulation technologies have achieved remarkable progress, social concerns about potential malicious abuse of these technologies bring out an emerging research topic of face forgery detection. However, it is extremely challenging since recent advances are able to forge faces beyond the perception ability of human eyes, especially in compressed images and videos. We ﬁnd that mining forgery patterns with the awareness of frequency could be a cure, as frequency provides a complementary viewpoint where either subtle forgery artifacts or compression errors could be well described. To introduce frequency into the face forgery detection, we propose a novel Frequency in Face Forgery Network (F3-Net), taking advantages of two diﬀerent but complementary frequency-aware clues, 1) frequency-aware decomposed image components, and 2) local frequency statistics, to deeply mine the forgery patterns via our two-stream collaborative learning framework. We apply DCT as the applied frequency-domain transformation. Through comprehensive studies, we show that the proposed F3-Net signiﬁcantly outperforms competing state-of-the-art methods on all compression qualities in the challenging FaceForensics++ dataset, especially wins a big lead upon low-quality media.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Qian, Yuyang and Yin, Guojun and Sheng, Lu and Chen, Zixuan and Shao, Jing},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_6},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {86--103},
	file = {Qian et al. - 2020 - Thinking in Frequency Face Forgery Detection by M.pdf:/Users/lucasjing/Zotero/storage/TH4896JI/Qian et al. - 2020 - Thinking in Frequency Face Forgery Detection by M.pdf:application/pdf},
}

@incollection{vedaldi_powering_2020,
	address = {Cham},
	title = {Powering {One}-{Shot} {Topological} {NAS} with {Stabilized} {Share}-{Parameter} {Proxy}},
	volume = {12359},
	isbn = {978-3-030-58567-9 978-3-030-58568-6},
	url = {https://link.springer.com/10.1007/978-3-030-58568-6_37},
	abstract = {One-shot NAS method has attracted much interest from the research community due to its remarkable training eﬃciency and capacity to discover high performance models. However, the search spaces of previous one-shot based works usually relied on hand-craft design and were short for ﬂexibility on the network topology. In this work, we try to enhance the one-shot NAS by exploring high-performing network architectures in our large-scale Topology Augmented Search Space (i.e, over 3.4 × 1010 diﬀerent topological structures). Speciﬁcally, the diﬃculties for architecture searching in such a complex space has been eliminated by the proposed stabilized share-parameter proxy, which employs Stochastic Gradient Langevin Dynamics to enable fast shared parameter sampling, so as to achieve stabilized measurement of architecture performance even in search space with complex topological structures. The proposed method, namely Stablized Topological Neural Architecture Search (ST-NAS), achieves state-of-the-art performance under Multiply-Adds (MAdds) constraint on ImageNet. Our lite model ST-NAS-A achieves 76.4\% top-1 accuracy with only 326M MAdds. Our moderate model STNAS-B achieves 77.9\% top-1 accuracy just required 503M MAdds. Both of our models oﬀer superior performances in comparison to other concurrent works on one-shot NAS.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Guo, Ronghao and Lin, Chen and Li, Chuming and Tian, Keyu and Sun, Ming and Sheng, Lu and Yan, Junjie},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58568-6_37},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {625--641},
	file = {Guo et al. - 2020 - Powering One-Shot Topological NAS with Stabilized .pdf:/Users/lucasjing/Zotero/storage/SUCJVH38/Guo et al. - 2020 - Powering One-Shot Topological NAS with Stabilized .pdf:application/pdf},
}

@article{sheng_high-quality_2020,
	title = {High-{Quality} {Video} {Generation} from {Static} {Structural} {Annotations}},
	volume = {128},
	issn = {1573-1405},
	url = {https://doi.org/10.1007/s11263-020-01334-x},
	doi = {10.1007/s11263-020-01334-x},
	abstract = {This paper proposes a novel unsupervised video generation that is conditioned on a single structural annotation map, which in contrast to prior conditioned video generation approaches, provides a good balance between motion flexibility and visual quality in the generation process. Different from end-to-end approaches that model the scene appearance and dynamics in a single shot, we try to decompose this difficult task into two easier sub-tasks in a divide-and-conquer fashion, thus achieving remarkable results overall. The first sub-task is an image-to-image (I2I) translation task that synthesizes high-quality starting frame from the input structural annotation map. The second image-to-video (I2V) generation task applies the synthesized starting frame and the associated structural annotation map to animate the scene dynamics for the generation of a photorealistic and temporally coherent video. We employ a cycle-consistent flow-based conditioned variational autoencoder to capture the long-term motion distributions, by which the learned bi-directional flows ensure the physical reliability of the predicted motions and provide explicit occlusion handling in a principled manner. Integrating structural annotations into the flow prediction also improves the structural awareness in the I2V generation process. Quantitative and qualitative evaluations over the autonomous driving and human action datasets demonstrate the effectiveness of the proposed approach over the state-of-the-art methods. The code has been released: https://github.com/junting/seg2vid.},
	language = {en},
	number = {10},
	urldate = {2022-08-22},
	journal = {International Journal of Computer Vision},
	author = {Sheng, Lu and Pan, Junting and Guo, Jiaming and Shao, Jing and Loy, Chen Change},
	month = nov,
	year = {2020},
	keywords = {Conditioned generative model, Image and video synthesis, Motion prediction and estimatiovn, Unsupervised learning},
	pages = {2552--2569},
}

@inproceedings{liu_morphing_2020,
	title = {Morphing and {Sampling} {Network} for {Dense} {Point} {Cloud} {Completion}},
	volume = {34},
	copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6827},
	doi = {10.1609/aaai.v34i07.6827},
	abstract = {3D point cloud completion, the task of inferring the complete geometric shape from a partial point cloud, has been attracting attention in the community. For acquiring high-fidelity dense point clouds and avoiding uneven distribution, blurred details, or structural loss of existing methods' results, we propose a novel approach to complete the partial point cloud in two stages. Specifically, in the first stage, the approach predicts a complete but coarse-grained point cloud with a collection of parametric surface elements. Then, in the second stage, it merges the coarse-grained prediction with the input point cloud by a novel sampling algorithm. Our method utilizes a joint loss function to guide the distribution of the points. Extensive experiments verify the effectiveness of our method and demonstrate that it outperforms the existing methods in both the Earth Mover's Distance (EMD) and the Chamfer Distance (CD).},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Proceedings of the {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Liu, Minghua and Sheng, Lu and Yang, Sheng and Shao, Jing and Hu, Shi-Min},
	month = apr,
	year = {2020},
	note = {Number: 07},
	pages = {11596--11603},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/U2TXH7Z8/Liu et al. - 2020 - Morphing and Sampling Network for Dense Point Clou.pdf:application/pdf},
}

@inproceedings{wang_camp_2019,
	address = {Seoul, Korea (South)},
	title = {{CAMP}: {Cross}-{Modal} {Adaptive} {Message} {Passing} for {Text}-{Image} {Retrieval}},
	isbn = {978-1-72814-803-8},
	shorttitle = {{CAMP}},
	url = {https://ieeexplore.ieee.org/document/9010827/},
	doi = {10.1109/ICCV.2019.00586},
	abstract = {Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Wang, Zihao and Liu, Xihui and Li, Hongsheng and Sheng, Lu and Yan, Junjie and Wang, Xiaogang and Shao, Jing},
	month = oct,
	year = {2019},
	pages = {5763--5772},
	file = {Wang et al. - 2019 - CAMP Cross-Modal Adaptive Message Passing for Tex.pdf:/Users/lucasjing/Zotero/storage/DYM4P696/Wang et al. - 2019 - CAMP Cross-Modal Adaptive Message Passing for Tex.pdf:application/pdf},
}

@inproceedings{pan_video_2019,
	address = {Long Beach, CA, USA},
	title = {Video {Generation} {From} {Single} {Semantic} {Label} {Map}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953551/},
	doi = {10.1109/CVPR.2019.00385},
	abstract = {This paper proposes the novel task of video generation conditioned on a SINGLE semantic label map, which provides a good balance between ﬂexibility and quality in the generation process. Different from typical end-to-end approaches, which model both scene content and dynamics in a single step, we propose to decompose this difﬁcult task into two sub-problems. As current image generation methods do better than video generation in terms of detail, we synthesize high quality content by only generating the ﬁrst frame. Then we animate the scene based on its semantic meaning to obtain temporally coherent video, giving us excellent results overall. We employ a cVAE for predicting optical ﬂow as a beneﬁcial intermediate step to generate a video sequence conditioned on the initial single frame. A semantic label map is integrated into the ﬂow prediction module to achieve major improvements in the image-to-video generation process. Extensive experiments on the Cityscapes dataset show that our method outperforms all competing methods. The source code will be released on https://github.com/junting/seg2vid.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pan, Junting and Wang, Chengyu and Jia, Xu and Shao, Jing and Sheng, Lu and Yan, Junjie and Wang, Xiaogang},
	month = jun,
	year = {2019},
	pages = {3728--3737},
	file = {Pan et al. - 2019 - Video Generation From Single Semantic Label Map.pdf:/Users/lucasjing/Zotero/storage/FDP7NVUN/Pan et al. - 2019 - Video Generation From Single Semantic Label Map.pdf:application/pdf},
}

@inproceedings{yin_context_2019,
	address = {Long Beach, CA, USA},
	title = {Context and {Attribute} {Grounded} {Dense} {Captioning}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8954079/},
	doi = {10.1109/CVPR.2019.00640},
	abstract = {Dense captioning aims at simultaneously localizing semantic regions and describing these regions-of-interest (ROIs) with short phrases or sentences in natural language. Previous studies have shown remarkable progresses, but they are often vulnerable to the aperture problem that a caption generated by the features inside one ROI lacks contextual coherence with its surrounding context in the input image. In this work, we investigate contextual reasoning based on multi-scale message propagations from the neighboring contents to the target ROIs. To this end, we design a novel end-to-end context and attribute grounded dense captioning framework consisting of 1) a contextual visual mining module and 2) a multi-level attribute grounded description generation module. Knowing that captions often co-occur with the linguistic attributes (such as who, what and where), we also incorporate an auxiliary supervision from hierarchical linguistic attributes to augment the distinctiveness of the learned captions. Extensive experiments and ablation studies on Visual Genome dataset demonstrate the superiority of the proposed model in comparison to the state-of-the-art methods.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
	month = jun,
	year = {2019},
	pages = {6234--6243},
	file = {Yin et al. - 2019 - Context and Attribute Grounded Dense Captioning.pdf:/Users/lucasjing/Zotero/storage/GKMA6G32/Yin et al. - 2019 - Context and Attribute Grounded Dense Captioning.pdf:application/pdf},
}

@inproceedings{liu_multi-label_2018,
	address = {New York, NY, USA},
	series = {{MM} '18},
	title = {Multi-{Label} {Image} {Classification} via {Knowledge} {Distillation} from {Weakly}-{Supervised} {Detection}},
	isbn = {978-1-4503-5665-7},
	url = {https://doi.org/10.1145/3240508.3240567},
	doi = {10.1145/3240508.3240567},
	abstract = {Multi-label image classification is a fundamental but challenging task towards general visual understanding. Existing methods found the region-level cues (e.g., features from RoIs) can facilitate multi-label classification. Nevertheless, such methods usually require laborious object-level annotations (i.e., object labels and bounding boxes) for effective learning of the object-level visual features. In this paper, we propose a novel and efficient deep framework to boost multi-label classification by distilling knowledge from weakly-supervised detection task without bounding box annotations. Specifically, given the image-level annotations, (1) we first develop a weakly-supervised detection (WSD) model, and then (2) construct an end-to-end multi-label image classification framework augmented by a knowledge distillation module that guides the classification model by the WSD model according to the class-level predictions for the whole image and the object-level visual features for object RoIs. The WSD model is the teacher model and the classification model is the student model. After this cross-task knowledge distillation, the performance of the classification model is significantly improved and the efficiency is maintained since the WSD model can be safely discarded in the test phase. Extensive experiments on two large-scale datasets (MS-COCO and NUS-WIDE) show that our framework achieves superior performances over the state-of-the-art methods on both performance and efficiency.},
	urldate = {2022-08-21},
	booktitle = {Proceedings of the 26th {ACM} international conference on {Multimedia}},
	publisher = {Association for Computing Machinery},
	author = {Liu, Yongcheng and Sheng, Lu and Shao, Jing and Yan, Junjie and Xiang, Shiming and Pan, Chunhong},
	year = {2018},
	keywords = {knowledge distillation, multi-label image classification, weakly-supervised detection},
	pages = {700--708},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/EFCHPNQ7/Liu et al. - 2018 - Multi-Label Image Classification via Knowledge Dis.pdf:application/pdf},
}

@incollection{ferrari_zoom-net_2018,
	address = {Cham},
	title = {Zoom-{Net}: {Mining} {Deep} {Feature} {Interactions} for {Visual} {Relationship} {Recognition}},
	volume = {11207},
	isbn = {978-3-030-01218-2 978-3-030-01219-9},
	shorttitle = {Zoom-{Net}},
	url = {http://link.springer.com/10.1007/978-3-030-01219-9_20},
	abstract = {Recognizing visual relationships subject-predicate-object among any pair of localized objects is pivotal for image understanding. Previous studies have shown remarkable progress in exploiting linguistic priors or external textual information to improve the performance. In this work, we investigate an orthogonal perspective based on feature interactions. We show that by encouraging deep message propagation and interactions between local object features and global predicate features, one can achieve compelling performance in recognizing complex relationships without using any linguistic priors. To this end, we present two new pooling cells to encourage feature interactions: (i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that inversely pools local object features to the corresponding area of global predicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global predicate features to reinforce local object features. The two cells constitute a Spatiality-Context-Appearance Module (SCA-M), which can be further stacked consecutively to form our ﬁnal Zoom-Net. We further shed light on how one could resolve ambiguous and noisy object and predicate annotations by Intra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual Genome dataset demonstrate the eﬀectiveness of our feature-oriented approach compared to state-of-the-art methods (Acc@1 11.42\% from 8.16\%) that depend on explicit modeling of linguistic interactions. We further show that SCA-M can be incorporated seamlessly into existing approaches to improve the performance by a large margin.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Yin, Guojun and Sheng, Lu and Liu, Bin and Yu, Nenghai and Wang, Xiaogang and Shao, Jing and Loy, Chen Change},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01219-9_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {330--347},
	file = {Yin et al. - 2018 - Zoom-Net Mining Deep Feature Interactions for Vis.pdf:/Users/lucasjing/Zotero/storage/5B8D377A/Yin et al. - 2018 - Zoom-Net Mining Deep Feature Interactions for Vis.pdf:application/pdf},
}

@inproceedings{liu_exploring_2018,
	address = {Salt Lake City, UT, USA},
	title = {Exploring {Disentangled} {Feature} {Representation} {Beyond} {Face} {Identification}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578320/},
	doi = {10.1109/CVPR.2018.00222},
	abstract = {This paper proposes learning disentangled but complementary face features with a minimal supervision by face identiﬁcation. Speciﬁcally, we construct an identity Distilling and Dispelling Autoencoder (D2AE) framework that adversarially learns the identity-distilled features for identity veriﬁcation and the identity-dispelled features to fool the veriﬁcation system. Thanks to the design of two-stream cues, the learned disentangled features represent not only the identity or attribute but the complete input image. Comprehensive evaluations further demonstrate that the proposed features not only preserve state-of-the-art identity veriﬁcation performance on LFW, but also acquire comparable discriminative power for face attribute recognition on CelebA and LFWA. Moreover, the proposed system is ready to semantically control the face generation/editing based on various identities and attributes in an unsupervised manner.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Liu, Yu and Wei, Fangyin and Shao, Jing and Sheng, Lu and Yan, Junjie and Wang, Xiaogang},
	month = jun,
	year = {2018},
	pages = {2080--2089},
	file = {Liu et al. - 2018 - Exploring Disentangled Feature Representation Beyo.pdf:/Users/lucasjing/Zotero/storage/6U3R3L6I/Liu et al. - 2018 - Exploring Disentangled Feature Representation Beyo.pdf:application/pdf},
}

@inproceedings{sheng_avatar-net_2018,
	address = {Salt Lake City, UT, USA},
	title = {Avatar-{Net}: {Multi}-scale {Zero}-{Shot} {Style} {Transfer} by {Feature} {Decoration}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Avatar-{Net}},
	url = {https://ieeexplore.ieee.org/document/8578958/},
	doi = {10.1109/CVPR.2018.00860},
	abstract = {Zero-shot artistic style transfer is an important image synthesis problem aiming at transferring arbitrary style into content images. However, the trade-off between the generalization and efﬁciency in existing methods impedes a high quality zero-shot style transfer in real-time. In this paper, we resolve this dilemma and propose an efﬁcient yet effective Avatar-Net that enables visually plausible multiscale transfer for arbitrary style. The key ingredient of our method is a style decorator that makes up the content features by semantically aligned style features from an arbitrary style image, which does not only holistically match their feature distributions but also preserve detailed style patterns in the decorated features. By embedding this module into an image reconstruction network that fuses multiscale style abstractions, the Avatar-Net renders multi-scale stylization for any style image in one feed-forward pass. We demonstrate the state-of-the-art effectiveness and efﬁciency of the proposed method in generating high-quality stylized images, with a series of successive applications include multiple style integration, video stylization and etc.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Sheng, Lu and Lin, Ziyi and Shao, Jing and Wang, Xiaogang},
	month = jun,
	year = {2018},
	pages = {8242--8250},
	file = {Sheng et al. - 2018 - Avatar-Net Multi-scale Zero-Shot Style Transfer b.pdf:/Users/lucasjing/Zotero/storage/D4NMCTNG/Sheng et al. - 2018 - Avatar-Net Multi-scale Zero-Shot Style Transfer b.pdf:application/pdf},
}

@inproceedings{yin_semantics_2019,
	title = {Semantics {Disentangling} for {Text}-{To}-{Image} {Generation}},
	doi = {10.1109/CVPR.2019.00243},
	abstract = {Synthesizing photo-realistic images from text descriptions is a challenging problem. Previous studies have shown remarkable progresses on visual quality of the generated images. In this paper, we consider semantics from the input text descriptions in helping render photo-realistic images. However, diverse linguistic expressions pose challenges in extracting consistent semantics even they depict the same thing. To this end, we propose a novel photo-realistic text-to-image generation model that implicitly disentangles semantics to both fulfill the high-level semantic consistency and low-level semantic diversity. To be specific, we design (1) a Siamese mechanism in the discriminator to learn consistent high-level semantics, and (2) a visual-semantic embedding strategy by semantic-conditioned batch normalization to find diverse low-level semantics. Extensive experiments and ablation studies on CUB and MS-COCO datasets demonstrate the superiority of the proposed method in comparison to state-of-the-art methods.},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Yin, Guojun and Liu, Bin and Sheng, Lu and Yu, Nenghai and Wang, Xiaogang and Shao, Jing},
	month = jun,
	year = {2019},
	note = {ISSN: 2575-7075},
	keywords = {Image and Video Synthesis, Vision + Language},
	pages = {2322--2331},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/F7SEVDTT/8953563.html:text/html;Yin et al. - Semantics Disentangling for Text-To-Image Generati.pdf:/Users/lucasjing/Zotero/storage/85LFTAGI/Yin et al. - Semantics Disentangling for Text-To-Image Generati.pdf:application/pdf},
}

@inproceedings{liu_hydraplus-net_2017,
	address = {Venice},
	title = {{HydraPlus}-{Net}: {Attentive} {Deep} {Features} for {Pedestrian} {Analysis}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {{HydraPlus}-{Net}},
	url = {http://ieeexplore.ieee.org/document/8237308/},
	doi = {10.1109/ICCV.2017.46},
	abstract = {Pedestrian analysis plays a vital role in intelligent video surveillance and is a key component for security-centric computer vision systems. Despite that the convolutional neural networks are remarkable in learning discriminative features from images, the learning of comprehensive features of pedestrians for fine-grained tasks remains an open problem. In this study, we propose a new attentionbased deep neural network, named as HydraPlus-Net (HPnet), that multi-directionally feeds the multi-level attention maps to different feature layers. The attentive deep features learned from the proposed HP-net bring unique advantages: (1) the model is capable of capturing multiple attentions from low-level to semantic-level, and (2) it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image. We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two tasks, i.e. pedestrian attribute recognition and person reidentification. Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-theart methods on various datasets.},
	language = {en},
	urldate = {2022-08-22},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Xihui and Zhao, Haiyu and Tian, Maoqing and Sheng, Lu and Shao, Jing and Yi, Shuai and Yan, Junjie and Wang, Xiaogang},
	month = oct,
	year = {2017},
	pages = {350--359},
	file = {Liu et al. - 2017 - HydraPlus-Net Attentive Deep Features for Pedestr.pdf:/Users/lucasjing/Zotero/storage/XNKBX5AP/Liu et al. - 2017 - HydraPlus-Net Attentive Deep Features for Pedestr.pdf:application/pdf},
}

@inproceedings{ma_mmekg_2022,
	address = {Dublin, Ireland},
	title = {{MMEKG}: {Multi}-modal {Event} {Knowledge} {Graph} towards {Universal} {Representation} across {Modalities}},
	shorttitle = {{MMEKG}},
	url = {https://aclanthology.org/2022.acl-demo.23},
	doi = {10.18653/v1/2022.acl-demo.23},
	abstract = {Events are fundamental building blocks of real-world happenings. In this paper, we present a large-scale, multi-modal event knowledge graph named MMEKG. MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.Specifically, MMEKG incorporates (i) over 990 thousand concept events with 644 relation types to cover most types of happenings, and (ii) over 863 million instance events connected through 934 million relations, which provide rich contextual information in texts and/or images. To collect billion-scale instance events and relations among them, we additionally develop an efficient yet effective pipeline for textual/visual knowledge extraction system. We also develop an induction strategy to create million-scale concept events and a schema organizing all events and relations in MMEKG. To this end, we also provide a pipeline enabling our system to seamlessly parse texts/images to event graphs and to retrieve multi-modal knowledge at both concept- and instance-levels.},
	urldate = {2022-08-26},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Yubo and Wang, Zehao and Li, Mukai and Cao, Yixin and Chen, Meiqi and Li, Xinze and Sun, Wenqi and Deng, Kunquan and Wang, Kun and Sun, Aixin and Shao, Jing},
	year = {2022},
	pages = {231--239},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/F4HMJB9U/Ma et al. - 2022 - MMEKG Multi-modal Event Knowledge Graph towards U.pdf:application/pdf},
}

@inproceedings{ma_prompt_2022,
	address = {Dublin, Ireland},
	title = {Prompt for {Extraction}? {PAIE}: {Prompting} {Argument} {Interaction} for {Event} {Argument} {Extraction}},
	shorttitle = {Prompt for {Extraction}?},
	url = {https://aclanthology.org/2022.acl-long.466},
	doi = {10.18653/v1/2022.acl-long.466},
	abstract = {In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data. On the one hand, PAIE utilizes prompt tuning for extractive objectives to take the best advantages of Pre-trained Language Models (PLMs). It introduces two span selectors based on the prompt to select start/end tokens among input texts for each role. On the other hand, it captures argument interactions via multi-role prompts and conducts joint optimization with optimal span assignments via a bipartite matching loss. Also, with a flexible prompt design, PAIE can extract multiple arguments with the same role instead of conventional heuristic threshold tuning. We have conducted extensive experiments on three benchmarks, including both sentence- and document-level EAE. The results present promising improvements from PAIE (3.5\% and 2.3\% F1 gains in average on three benchmarks, for PAIE-base and PAIE-large respectively). Further analysis demonstrates the efficiency, generalization to few-shot settings, and effectiveness of different extractive prompt tuning strategies. Our code is available at https://github.com/mayubo2333/PAIE.},
	urldate = {2022-08-26},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Yubo and Wang, Zehao and Cao, Yixin and Li, Mukai and Chen, Meiqi and Wang, Kun and Shao, Jing},
	year = {2022},
	pages = {6759--6774},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/3KRXNKKQ/Ma et al. - 2022 - Prompt for Extraction PAIE Prompting Argument In.pdf:application/pdf},
}

@inproceedings{li_supervision_2022,
	title = {Supervision {Exists} {Everywhere}: {A} {Data} {Efficient} {Contrastive} {Language}-{Image} {Pre}-training {Paradigm}},
	shorttitle = {Supervision {Exists} {Everywhere}},
	url = {https://openreview.net/forum?id=zq1iJkNk3uN},
	abstract = {Recently, large-scale Contrastive Language-Image Pre-training (CLIP) has attracted unprecedented attention for its impressive zero-shot recognition ability and excellent transferability to...},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Li, Yangguang and Liang, Feng and Zhao, Lichen and Cui, Yufeng and Ouyang, Wanli and Shao, Jing and Yu, Fengwei and Yan, Junjie},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/MU24JGVI/Li et al. - 2022 - Supervision Exists Everywhere A Data Efficient Co.pdf:application/pdf;Snapshot:/Users/lucasjing/Zotero/storage/BYYZ3P2C/forum.html:text/html},
}

@inproceedings{wang_repre_2022,
	title = {{RePre}: {Improving} {Self}-{Supervised} {Vision} {Transformer} with {Reconstructive} {Pre}-training},
	volume = {2},
	shorttitle = {{RePre}},
	url = {https://www.ijcai.org/proceedings/2022/200},
	doi = {10.24963/ijcai.2022/200},
	abstract = {Electronic proceedings of IJCAI 2022},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Wang, Luya and Liang, Feng and Li, Yangguang and Zhang, Honggang and Ouyang, Wanli and Shao, Jing},
	month = jul,
	year = {2022},
	note = {ISSN: 1045-0823},
	pages = {1437--1443},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/V7HBTGDV/Wang et al. - 2022 - RePre Improving Self-Supervised Vision Transforme.pdf:application/pdf;Snapshot:/Users/lucasjing/Zotero/storage/9KCC257T/200.html:text/html},
}

@article{zhong_blockqnn_2021,
	title = {{BlockQNN}: {Efficient} {Block}-{Wise} {Neural} {Network} {Architecture} {Generation}},
	volume = {43},
	issn = {1939-3539},
	shorttitle = {{BlockQNN}},
	doi = {10.1109/TPAMI.2020.2969193},
	abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most popular network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained to choose component layers sequentially. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it yields state-of-the-art results in comparison to the hand-crafted networks on image classification, particularly, the best network generated by BlockQNN achieves 2.35 percent top-1 error rate on CIFAR-10. (2) it offers tremendous reduction of the search space in designing networks, spending only 3 days with 32 GPUs. A faster version can yield a comparable result with only 1 GPU in 20 hours. (3) it has strong generalizability in that the network built on CIFAR also performs well on the larger-scale dataset. The best network achieves very competitive accuracy of 82.0 percent top-1 and 96.0 percent top-5 on ImageNet.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhong, Zhao and Yang, Zichen and Deng, Boyang and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
	month = jul,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Acceleration, AutoML, Computer architecture, Convolutional neural network, Graphics processing units, Indexes, Network architecture, neural architecture search, Neural networks, Q-learning, reinforcement learning, Task analysis},
	pages = {2314--2328},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/RTI8PPA5/8966988.html:text/html;Submitted Version:/Users/lucasjing/Zotero/storage/XGTIUGPG/Zhong et al. - 2021 - BlockQNN Efficient Block-Wise Neural Network Arch.pdf:application/pdf},
}

@incollection{vedaldi_celeba-spoof_2020,
	address = {Cham},
	title = {{CelebA}-{Spoof}: {Large}-{Scale} {Face} {Anti}-spoofing {Dataset} with {Rich} {Annotations}},
	volume = {12357},
	isbn = {978-3-030-58609-6 978-3-030-58610-2},
	shorttitle = {{CelebA}-{Spoof}},
	url = {https://link.springer.com/10.1007/978-3-030-58610-2_5},
	abstract = {As facial interaction systems are prevalently deployed, security and reliability of these systems become a critical issue, with substantial research eﬀorts devoted. Among them, face anti-spooﬁng emerges as an important area, whose objective is to identify whether a presented face is live or spoof. Though promising progress has been achieved, existing works still have diﬃculty in handling complex spoof attacks and generalizing to real-world scenarios. The main reason is that current face anti-spooﬁng datasets are limited in both quantity and diversity. To overcome these obstacles, we contribute a large-scale face anti-spooﬁng dataset, CelebA-Spoof, with the following appealing properties: 1) Quantity: CelebA-Spoof comprises of 625,537 pictures of 10,177 subjects, signiﬁcantly larger than the existing datasets. 2) Diversity: The spoof images are captured from 8 scenes (2 environments * 4 illumination conditions) with more than 10 sensors. 3) Annotation Richness: CelebA-Spoof contains 10 spoof type annotations, as well as the 40 attribute annotations inherited from the original CelebA dataset. Equipped with CelebA-Spoof, we carefully benchmark existing methods in a uniﬁed multi-task framework, Auxiliary Information Embedding Network (AENet), and reveal several valuable observations. Our key insight is that, compared with the commonly-used binary supervision or mid-level geometric representations, rich semantic annotations as auxiliary tasks can greatly boost the performance and generalizability of face anti-spooﬁng across a wide range of spoof attacks. Through comprehensive studies, we show that CelebA-Spoof serves as an eﬀective training data source. Models trained on CelebA-Spoof (without ﬁne-tuning) exhibit state-of-the-art performance on standard benchmarks such as CASIA-MFSD. The datasets are available at https://github.com/Davidzhangyuanhan/CelebA-Spoof .},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Zhang, Yuanhan and Yin, ZhenFei and Li, Yidong and Yin, Guojun and Yan, Junjie and Shao, Jing and Liu, Ziwei},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58610-2_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {70--85},
	file = {Zhang et al. - 2020 - CelebA-Spoof Large-Scale Face Anti-spoofing Datas.pdf:/Users/lucasjing/Zotero/storage/LE6PLREP/Zhang et al. - 2020 - CelebA-Spoof Large-Scale Face Anti-spoofing Datas.pdf:application/pdf},
}

@incollection{vedaldi_learning_2020,
	address = {Cham},
	title = {Learning {Connectivity} of {Neural} {Networks} from a {Topological} {Perspective}},
	volume = {12366},
	isbn = {978-3-030-58588-4 978-3-030-58589-1},
	url = {https://link.springer.com/10.1007/978-3-030-58589-1_44},
	abstract = {Seeking eﬀective neural networks is a critical and practical ﬁeld in deep learning. Besides designing the depth, type of convolution, normalization, and nonlinearities, the topological connectivity of neural networks is also important. Previous principles of rule-based modular design simplify the diﬃculty of building an eﬀective architecture, but constrain the possible topologies in limited spaces. In this paper, we attempt to optimize the connectivity in neural networks. We propose a topological perspective to represent a network into a complete graph for analysis, where nodes carry out aggregation and transformation of features, and edges determine the ﬂow of information. By assigning learnable parameters to the edges which reﬂect the magnitude of connections, the learning process can be performed in a diﬀerentiable manner. We further attach auxiliary sparsity constraint to the distribution of connectedness, which promotes the learned topology focus on critical connections. This learning process is compatible with existing networks and owns adaptability to larger search spaces and diﬀerent tasks. Quantitative results of experiments reﬂect the learned connectivity is superior to traditional rule-based ones, such as random, residual and complete. In addition, it obtains signiﬁcant improvements in image classiﬁcation and object detection without introducing excessive computation burden.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {Computer {Vision} – {ECCV} 2020},
	publisher = {Springer International Publishing},
	author = {Yuan, Kun and Li, Quanquan and Shao, Jing and Yan, Junjie},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	doi = {10.1007/978-3-030-58589-1_44},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {737--753},
	file = {Yuan et al. - 2020 - Learning Connectivity of Neural Networks from a To.pdf:/Users/lucasjing/Zotero/storage/BJR6JP6I/Yuan et al. - 2020 - Learning Connectivity of Neural Networks from a To.pdf:application/pdf},
}

@inproceedings{liu_improving_2019,
	address = {Long Beach, CA, USA},
	title = {Improving {Referring} {Expression} {Grounding} {With} {Cross}-{Modal} {Attention}-{Guided} {Erasing}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953898/},
	doi = {10.1109/CVPR.2019.00205},
	abstract = {Referring expression grounding aims at locating certain objects or persons in an image with a referring expression, where the key challenge is to comprehend and align various types of information from visual and textual domain, such as visual attributes, location and interactions with surrounding regions. Although the attention mechanism has been successfully applied for cross-modal alignments, previous attention models focus on only the most dominant features of both modalities, and neglect the fact that there could be multiple comprehensive textual-visual correspondences between images and referring expressions. To tackle this issue, we design a novel cross-modal attention-guided erasing approach, where we discard the most dominant information from either textual or visual domains to generate difﬁcult training samples online, and to drive the model to discover complementary textual-visual correspondences. Extensive experiments demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance on three referring expression grounding datasets.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Liu, Xihui and Wang, Zihao and Shao, Jing and Wang, Xiaogang and Li, Hongsheng},
	month = jun,
	year = {2019},
	pages = {1950--1959},
	file = {Liu et al. - 2019 - Improving Referring Expression Grounding With Cros.pdf:/Users/lucasjing/Zotero/storage/SQSQRSSL/Liu et al. - 2019 - Improving Referring Expression Grounding With Cros.pdf:application/pdf},
}

@inproceedings{liu_learning_2019,
	title = {Learning to {Predict} {Layout}-to-image {Conditional} {Convolutions} for {Semantic} {Image} {Synthesis}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/b2eb7349035754953b57a32e2841bda5-Abstract.html},
	abstract = {Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.},
	urldate = {2022-08-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Xihui and Yin, Guojun and Shao, Jing and Wang, Xiaogang and Li, hongsheng},
	year = {2019},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/CSCQ2C5R/Liu et al. - 2019 - Learning to Predict Layout-to-image Conditional Co.pdf:application/pdf},
}

@inproceedings{liu_localization_2018,
	title = {Localization {Guided} {Learning} for {Pedestrian} {Attribute} {Recognition}},
	url = {https://scholars.cityu.edu.hk/en/publications/publication(2d611d83-0f81-45ac-a8d6-243d390d50b4).html},
	language = {English},
	urldate = {2022-08-26},
	booktitle = {British {Machine} {Vision} {Conference} 2018, {BMVC} 2018},
	publisher = {BMVA Press},
	author = {Liu, Pengze and Liu, Xihui and Yan, Junjie and Shao, Jing},
	month = sep,
	year = {2018},
	file = {Snapshot:/Users/lucasjing/Zotero/storage/FEQCY28N/publication(2d611d83-0f81-45ac-a8d6-243d390d50b4).html:text/html},
}

@inproceedings{zhong_practical_2018,
	address = {Salt Lake City, UT},
	title = {Practical {Block}-{Wise} {Neural} {Network} {Architecture} {Generation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578355/},
	doi = {10.1109/CVPR.2018.00257},
	abstract = {Convolutional neural networks have gained a remarkable success in computer vision. However, most usable network architectures are hand-crafted and usually require expertise and elaborate design. In this paper, we provide a block-wise network generation pipeline called BlockQNN which automatically builds high-performance networks using the Q-Learning paradigm with epsilon-greedy exploration strategy. The optimal network block is constructed by the learning agent which is trained sequentially to choose component layers. We stack the block to construct the whole auto-generated network. To accelerate the generation process, we also propose a distributed asynchronous framework and an early stop strategy. The block-wise generation brings unique advantages: (1) it performs competitive results in comparison to the hand-crafted state-of-the-art networks on image classiﬁcation, additionally, the best network generated by BlockQNN achieves 3.54\% top-1 error rate on CIFAR-10 which beats all existing auto-generate networks. (2) in the meanwhile, it offers tremendous reduction of the search space in designing networks which only spends 3 days with 32 GPUs, and (3) moreover, it has strong generalizability that the network built on CIFAR also performs well on a larger-scale ImageNet dataset.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhong, Zhao and Yan, Junjie and Wu, Wei and Shao, Jing and Liu, Cheng-Lin},
	month = jun,
	year = {2018},
	pages = {2423--2432},
	file = {Zhong et al. - 2018 - Practical Block-Wise Neural Network Architecture G.pdf:/Users/lucasjing/Zotero/storage/MJ7NSR4A/Zhong et al. - 2018 - Practical Block-Wise Neural Network Architecture G.pdf:application/pdf},
}

@incollection{ferrari_transductive_2018,
	address = {Cham},
	title = {Transductive {Centroid} {Projection} for {Semi}-supervised {Large}-{Scale} {Recognition}},
	volume = {11209},
	isbn = {978-3-030-01227-4 978-3-030-01228-1},
	url = {http://link.springer.com/10.1007/978-3-030-01228-1_5},
	abstract = {Conventional deep semi-supervised learning methods, such as recursive clustering and training process, suﬀer from cumulative error and high computational complexity when collaborating with Convolutional Neural Networks. To this end, we design a simple but eﬀective learning mechanism that merely substitutes the last fully-connected layer with the proposed Transductive Centroid Projection (TCP) module. It is inspired by the observation of the weights in the ﬁnal classiﬁcation layer (called anchors) converge to the central direction of each class in hyperspace. Speciﬁcally, we design the TCP module by dynamically adding an ad hoc anchor for each cluster in one mini-batch. It essentially reduces the probability of the inter-class conﬂict and enables the unlabelled data functioning as labelled data. We inspect its eﬀectiveness with elaborate ablation study on seven public face/person classiﬁcation benchmarks. Without any bells and whistles, TCP can achieve signiﬁcant performance gains over most state-of-the-art methods in both fully-supervised and semi-supervised manners.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liu, Yu and Song, Guanglu and Shao, Jing and Jin, Xiao and Wang, Xiaogang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01228-1_5},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {72--89},
	file = {Liu et al. - 2018 - Transductive Centroid Projection for Semi-supervis.pdf:/Users/lucasjing/Zotero/storage/N7ZBU8YK/Liu et al. - 2018 - Transductive Centroid Projection for Semi-supervis.pdf:application/pdf},
}

@incollection{ferrari_show_2018,
	address = {Cham},
	title = {Show, {Tell} and {Discriminate}: {Image} {Captioning} by {Self}-retrieval with {Partially} {Labeled} {Data}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	shorttitle = {Show, {Tell} and {Discriminate}},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_21},
	abstract = {The aim of image captioning is to generate captions by machine to describe image contents. Despite many eﬀorts, generating discriminative captions for images remains non-trivial. Most traditional approaches imitate the language structure patterns, thus tend to fall into a stereotype of replicating frequent phrases or sentences and neglect unique aspects of each image. In this work, we propose an image captioning framework with a self-retrieval module as training guidance, which encourages generating discriminative captions. It brings unique advantages: (1) the self-retrieval guidance can act as a metric and an evaluator of caption discriminativeness to assure the quality of generated captions. (2) The correspondence between generated captions and images are naturally incorporated in the generation process without human annotations, and hence our approach could utilize a large amount of unlabeled images to boost captioning performance with no additional annotations. We demonstrate the eﬀectiveness of the proposed retrievalguided method on COCO and Flickr30k captioning datasets, and show its superior captioning performance with more discriminative captions.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Liu, Xihui and Li, Hongsheng and Shao, Jing and Chen, Dapeng and Wang, Xiaogang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01267-0_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {353--369},
	file = {Liu et al. - 2018 - Show, Tell and Discriminate Image Captioning by S.pdf:/Users/lucasjing/Zotero/storage/ALCWX5BN/Liu et al. - 2018 - Show, Tell and Discriminate Image Captioning by S.pdf:application/pdf},
}

@article{shao_learning_2017,
	title = {Learning {Scene}-{Independent} {Group} {Descriptors} for {Crowd} {Understanding}},
	volume = {27},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2016.2539878},
	abstract = {Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this paper, we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel collective transition prior, which leads to a robust approach for group segregation in public spaces. From the former, we further devise a rich set of group-property visual descriptors. These descriptors are scene-independent and can be effectively applied to public scenes with a variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are complementary to each other, scene-independent, and they convey critical information on physical states of a crowd. The proposed group-level descriptors show promising results and potentials in multiple applications, including crowd dynamic monitoring, crowd video classification, and crowd video retrieval.},
	number = {6},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Shao, Jing and Loy, Chen Change and Wang, Xiaogang},
	month = jun,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Circuit stability, Crowded scene understanding, Feature extraction, group-property analysis, Hidden Markov models, Psychology, Robustness, Stability analysis, video analysis, Visualization},
	pages = {1290--1303},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/DIMYR8TU/7428859.html:text/html},
}

@incollection{ferrari_improving_2018,
	address = {Cham},
	title = {Improving {Deep} {Visual} {Representation} for {Person} {Re}-identification by {Global} and {Local} {Image}-language {Association}},
	volume = {11220},
	isbn = {978-3-030-01269-4 978-3-030-01270-0},
	url = {http://link.springer.com/10.1007/978-3-030-01270-0_4},
	abstract = {Person re-identiﬁcation is an important task that requires learning discriminative visual features for distinguishing diﬀerent person identities. Diverse auxiliary information has been utilized to improve the visual feature learning. In this paper, we propose to exploit natural language description as additional training supervisions for eﬀective visual features. Compared with other auxiliary information, language can describe a speciﬁc person from more compact and semantic visual aspects, thus is complementary to the pixel-level image data. Our method not only learns better global visual feature with the supervision of the overall description but also enforces semantic consistencies between local visual and linguistic features, which is achieved by building global and local image-language associations. The global image-language association is established according to the identity labels, while the local association is based upon the implicit correspondences between image regions and noun phrases. Extensive experiments demonstrate the eﬀectiveness of employing language as training supervisions with the two association schemes. Our method achieves state-of-the-art performance without utilizing any auxiliary information during testing and shows better performance than other joint embedding methods for the image-language association.},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Chen, Dapeng and Li, Hongsheng and Liu, Xihui and Shen, Yantao and Shao, Jing and Yuan, Zejian and Wang, Xiaogang},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01270-0_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {56--73},
	file = {Chen et al. - 2018 - Improving Deep Visual Representation for Person Re.pdf:/Users/lucasjing/Zotero/storage/D6K53E4U/Chen et al. - 2018 - Improving Deep Visual Representation for Person Re.pdf:application/pdf},
}

@article{shao_crowded_2017,
	title = {Crowded {Scene} {Understanding} by {Deeply} {Learned} {Volumetric} {Slices}},
	volume = {27},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2016.2593647},
	abstract = {Crowd video analysis is one of the hallmark tasks of crowded scene understanding. While we observe a tremendous progress in image-based tasks with the rise of convolutional neural networks (CNNs), performance on video analysis has not (yet) attained the same level of success. In this paper, we introduce intuitive but effective temporal-aware crowd motion channels by uniformly slicing the video volume from different dimensions. Multiple CNN structures with different data-fusion strategies and weight-sharing schemes are proposed to learn the connectivity both spatially and temporally from these motion channels. To well demonstrate our deep model, we construct a new large-scale Who do What at someWhere crowd data set with 10 000 videos from 8257 crowded scenes, and build an attribute set with 94 attributes. Extensive experiments on crowd video attribute prediction demonstrate the effectiveness of our novel method over the state-of-the-art.},
	number = {3},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Shao, Jing and Loy, Chen Change and Kang, Kai and Wang, Xiaogang},
	month = mar,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Crowd database, crowded scene understanding, deep neural network, Feature extraction, Image segmentation, Motion segmentation, Neural networks, Semantics, spatiotemporal features, Surveillance, video analysis, World Wide Web},
	pages = {613--623},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/BTUVMI4P/7517290.html:text/html},
}

@inproceedings{zhao_spindle_2017,
	address = {Honolulu, HI, USA},
	title = {Spindle {Net}: {Person} {Re}-identification with {Human} {Body} {Region} {Guided} {Feature} {Decomposition} and {Fusion}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Spindle {Net}},
	url = {https://ieeexplore.ieee.org/document/8099586/},
	doi = {10.1109/CVPR.2017.103},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
	month = jul,
	year = {2017},
	pages = {907--915},
	file = {Zhao et al. - 2017 - Spindle Net Person Re-identification with Human B.pdf:/Users/lucasjing/Zotero/storage/EGQM6K7C/Zhao et al. - 2017 - Spindle Net Person Re-identification with Human B.pdf:application/pdf},
}

@inproceedings{wang_orientation_2017,
	title = {Orientation {Invariant} {Feature} {Embedding} and {Spatial} {Temporal} {Regularization} for {Vehicle} {Re}-identification},
	doi = {10.1109/ICCV.2017.49},
	abstract = {In this paper, we tackle the vehicle Re-identification (ReID) problem which is of great importance in urban surveillance and can be used for multiple applications. In our vehicle ReID framework, an orientation invariant feature embedding module and a spatial-temporal regularization module are proposed. With orientation invariant feature embedding, local region features of different orientations can be extracted based on 20 key point locations and can be well aligned and combined. With spatial-temporal regularization, the log-normal distribution is adopted to model the spatial-temporal constraints and the retrieval results can be refined. Experiments are conducted on public vehicle ReID datasets and our proposed method achieves state-of-the-art performance. Investigations of the proposed framework is conducted, including the landmark regressor and comparisons with attention mechanism. Both the orientation invariant feature embedding and the spatio-temporal regularization achieve considerable improvements.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wang, Zhongdao and Tang, Luming and Liu, Xihui and Yao, Zhuliang and Yi, Shuai and Shao, Jing and Yan, Junjie and Wang, Shengjin and Li, Hongsheng and Wang, Xiaogang},
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Automobiles, Feature extraction, Pipelines, Proposals, Surveillance, Wheels},
	pages = {379--387},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/IV7PCI6R/8237311.html:text/html},
}

@inproceedings{shao_slicing_2016,
	title = {Slicing {Convolutional} {Neural} {Network} for {Crowd} {Video} {Understanding}},
	doi = {10.1109/CVPR.2016.606},
	abstract = {Learning and capturing both appearance and dynamic representations are pivotal for crowd video understanding. Convolutional Neural Networks (CNNs) have shown its remarkable potential in learning appearance representations from images. However, the learning of dynamic representation, and how it can be effectively combined with appearance features for video analysis, remains an open problem. In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatio-and 2D temporal-slices representations. The decomposition brings unique advantages: (1) the model is capable of capturing dynamics of different semantic units such as groups and objects, (2) it learns separated appearance and dynamic representations while keeping proper interactions between them, and (3) it exploits the selectiveness of spatial filters to discard irrelevant background clutter for crowd understanding. We demonstrate the effectiveness of the proposed S-CNN model on the WWW crowd video dataset for attribute recognition and observe significant performance improvements to the state-of-the-art methods (62.55\% from 51.84\% [21]).},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Shao, Jing and Loy, Chen Change and Kang, Kai and Wang, Xiaogang},
	month = jun,
	year = {2016},
	note = {ISSN: 1063-6919},
	keywords = {Dynamics, Feature extraction, Ice, Semantics, Three-dimensional displays, Two dimensional displays, Visualization},
	pages = {5620--5628},
	file = {IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/DKIDBWAV/7780975.html:text/html},
}

@inproceedings{shao_scene-independent_2014,
	title = {Scene-{Independent} {Group} {Profiling} in {Crowd}},
	doi = {10.1109/CVPR.2014.285},
	abstract = {Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this study we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel Collective Transition prior, which leads to a robust approach for group segregation in public spaces. From the prior, we further devise a rich set of group property visual descriptors. These descriptors are scene-independent, and can be effectively applied to public-scene with variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are not only useful but also necessary for group state analysis and crowd scene understanding.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Shao, Jing and Loy, Chen Change and Wang, Xiaogang},
	month = jun,
	year = {2014},
	note = {ISSN: 1063-6919},
	keywords = {Computed tomography, Context, Robustness, Stability criteria, Tracking, Visualization},
	pages = {2227--2234},
	file = {Full Text PDF:/Users/lucasjing/Zotero/storage/JW63Z2Y3/Shao et al. - 2014 - Scene-Independent Group Profiling in Crowd.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/lucasjing/Zotero/storage/RHMYXNMT/6909682.html:text/html;Snapshot:/Users/lucasjing/Zotero/storage/M3USFBE5/Shao_Scene-Independent_Group_Profiling_2014_CVPR_paper.html:text/html},
}

@inproceedings{shao_deeply_2015,
	title = {Deeply learned attributes for crowded scene understanding},
	doi = {10.1109/CVPR.2015.7299097},
	abstract = {Crowded scene understanding is a fundamental problem in computer vision. In this study, we develop a multi-task deep model to jointly learn and combine appearance and motion features for crowd understanding. We propose crowd motion channels as the input of the deep model and the channel design is inspired by generic properties of crowd systems. To well demonstrate our deep model, we construct a new large-scale WWW Crowd dataset with 10, 000 videos from 8, 257 crowded scenes, and build an attribute set with 94 attributes on WWW. We further measure user study performance on WWW and compare this with the proposed deep models. Extensive experiments show that our deep models display significant performance improvements in cross-scene attribute recognition compared to strong crowd-related feature-based baselines, and the deeply learned features behave a superior performance in multi-task learning.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Shao, Jing and Kang, Kai and Loy, Chen Change and Wang, Xiaogang},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Accuracy, Computational modeling, Stability analysis, Time factors, Tracking, Videos, World Wide Web},
	pages = {4657--4666},
}

@article{wang_sncse_2022,
	title = {{SNCSE}: {Contrastive} {Learning} for {Unsupervised} {Sentence} {Embedding} with {Soft} {Negative} {Samples}},
	volume = {abs/2201.05979},
	shorttitle = {{SNCSE}},
	journal = {CoRR},
	author = {Wang, Hao and Li, Yangguang and Huang, Zhen and Dou, Yong and Kong, Lingpeng and Shao, Jing},
	year = {2022},
}

@article{cui_democratizing_2022,
	title = {Democratizing {Contrastive} {Language}-{Image} {Pre}-training: {A} {CLIP} {Benchmark} of {Data}, {Model}, and {Supervision}},
	volume = {abs/2203.05796},
	shorttitle = {Democratizing {Contrastive} {Language}-{Image} {Pre}-training},
	journal = {CoRR},
	author = {Cui, Yufeng and Zhao, Lichen and Liang, Feng and Li, Yangguang and Shao, Jing},
	year = {2022},
}

@article{zhang_bamboo_2022,
	title = {Bamboo: {Building} {Mega}-{Scale} {Vision} {Dataset} {Continually} with {Human}-{Machine} {Synergy}},
	volume = {abs/2203.07845},
	shorttitle = {Bamboo},
	journal = {CoRR},
	author = {Zhang, Yuanhan and Sun, Qinghong and Zhou, Yichun and He, Zexin and Yin, Zhenfei and Wang, Kun and Sheng, Lu and Qiao, Yu and Shao, Jing and Liu, Ziwei},
	year = {2022},
}

@article{he_x-learner_2022,
	title = {X-{Learner}: {Learning} {Cross} {Sources} and {Tasks} for {Universal} {Visual} {Representation}},
	volume = {abs/2203.08764},
	shorttitle = {X-{Learner}},
	journal = {CoRR},
	author = {He, Yinan and Huang, Gengshi and Chen, Siyu and Teng, Jianing and Kun, Wang and Yin, Zhenfei and Sheng, Lu and Liu, Ziwei and Qiao, Yu and Shao, Jing},
	year = {2022},
}

@article{qiu_few-shot_2022,
	title = {Few-shot {Forgery} {Detection} via {Guided} {Adversarial} {Interpolation}},
	volume = {abs/2204.05905},
	journal = {CoRR},
	author = {Qiu, Haonan and Chen, Siyu and Gan, Bei and Wang, Kun and Shi, Huafeng and Shao, Jing and Liu, Ziwei},
	year = {2022},
}

@article{chen_ergo_2022,
	title = {{ERGO}: {Event} {Relational} {Graph} {Transformer} for {Document}-level {Event} {Causality} {Identification}},
	volume = {abs/2204.07434},
	shorttitle = {{ERGO}},
	journal = {CoRR},
	author = {Chen, Meiqi and Cao, Yixin and Deng, Kunquan and Li, Mukai and Wang, Kun and Shao, Jing and Zhang, Yan},
	year = {2022},
}

@article{zhang_robust_2022,
	title = {Robust {Face} {Anti}-{Spoofing} with {Dual} {Probabilistic} {Modeling}},
	volume = {abs/2204.12685},
	journal = {CoRR},
	author = {Zhang, Yuanhan and Wu, Yichao and Yin, Zhenfei and Shao, Jing and Liu, Ziwei},
	year = {2022},
}

@article{an_1st_2022,
	title = {1st {Place} {Solutions} for {RxR}-{Habitat} {Vision}-and-{Language} {Navigation} {Competition} ({CVPR} 2022)},
	volume = {abs/2206.11610},
	journal = {CoRR},
	author = {An, Dong and Wang, Zun and Li, Yangguang and Wang, Yi and Hong, Yicong and Huang, Yan and Wang, Liang and Shao, Jing},
	year = {2022},
}

@article{pan_st-adapter_2022,
	title = {{ST}-{Adapter}: {Parameter}-{Efficient} {Image}-to-{Video} {Transfer} {Learning} for {Action} {Recognition}},
	volume = {abs/2206.13559},
	shorttitle = {{ST}-{Adapter}},
	journal = {CoRR},
	author = {Pan, Junting and Lin, Ziyi and Zhu, Xiatian and Shao, Jing and Li, Hongsheng},
	year = {2022},
}

@article{zhang_benchmarking_2022,
	title = {Benchmarking {Omni}-{Vision} {Representation} through the {Lens} of {Visual} {Realms}},
	volume = {abs/2207.07106},
	journal = {CoRR},
	author = {Zhang, Yuanhan and Yin, Zhenfei and Shao, Jing and Liu, Ziwei},
	year = {2022},
}

@article{tang_task-balanced_2022,
	title = {Task-{Balanced} {Distillation} for {Object} {Detection}},
	volume = {abs/2208.03006},
	journal = {CoRR},
	author = {Tang, Ruining and Liu, Zhenyu and Li, Yangguang and Song, Yiguo and Liu, Hui and Wang, Qide and Shao, Jing and Duan, Guifang and Tan, Jianrong},
	year = {2022},
}

@article{yang_few-shot_2021,
	title = {Few-{Shot} {Domain} {Expansion} for {Face} {Anti}-{Spoofing}},
	volume = {abs/2106.14162},
	journal = {CoRR},
	author = {Yang, Bowen and Zhang, Jing and Yin, Zhenfei and Shao, Jing},
	year = {2021},
}

@article{shao_intern_2021,
	title = {{INTERN}: {A} {New} {Learning} {Paradigm} {Towards} {General} {Vision}},
	volume = {abs/2111.08687},
	shorttitle = {{INTERN}},
	journal = {CoRR},
	author = {Shao, Jing and Chen, Siyu and Li, Yangguang and Wang, Kun and Yin, Zhenfei and He, Yinan and Teng, Jianing and Sun, Qinghong and Gao, Mengya and Liu, Jihao and Huang, Gengshi and Song, Guanglu and Wu, Yichao and Huang, Yuming and Liu, Fenggang and Peng, Huan and Qin, Shuo and Wang, Chengyu and Wang, Yujie and He, Conghui and Liang, Ding and Liu, Yu and Yu, Fengwei and Yan, Junjie and Lin, Dahua and Wang, Xiaogang and Qiao, Yu},
	year = {2021},
}

@article{ma_simple_2021,
	title = {A {Simple} {Long}-{Tailed} {Recognition} {Baseline} via {Vision}-{Language} {Model}},
	volume = {abs/2111.14745},
	journal = {CoRR},
	author = {Ma, Teli and Geng, Shijie and Wang, Mengmeng and Shao, Jing and Lu, Jiasen and Li, Hongsheng and Gao, Peng and Qiao, Yu},
	year = {2021},
}

@article{he_forgerynet_2021-1,
	title = {{ForgeryNet} - {Face} {Forgery} {Analysis} {Challenge} 2021: {Methods} and {Results}},
	volume = {abs/2112.08325},
	shorttitle = {{ForgeryNet} - {Face} {Forgery} {Analysis} {Challenge} 2021},
	journal = {CoRR},
	author = {He, Yinan and Sheng, Lu and Shao, Jing and Liu, Ziwei and Zou, Zhaofan and Guo, Zhizhi and Jiang, Shan and Sun, Curitis and Zhang, Guosheng and Wang, Keyao and Yue, Haixiao and Hong, Zhibin and Wang, Wanguo and Li, Zhenyu and Wang, Qi and Wang, Zhenli and Xu, Ronghao and Zhang, Mingwen and Wang, Zhiheng and Huang, Zhenhang and Zhang, Tianming and Zhao, Ningning},
	year = {2021},
}

@article{chen_1st_2020,
	title = {1st place solution for {AVA}-{Kinetics} {Crossover} in {AcitivityNet} {Challenge} 2020},
	volume = {abs/2006.09116},
	journal = {CoRR},
	author = {Chen, Siyu and Pan, Junting and Song, Guanglu and Zhang, Manyuan and Shao, Hao and Lin, Ziyi and Shao, Jing and Li, Hongsheng and Liu, Yu},
	year = {2020},
}

@article{wang_pv-nas_2020,
	title = {{PV}-{NAS}: {Practical} {Neural} {Architecture} {Search} for {Video} {Recognition}},
	volume = {abs/2011.00826},
	shorttitle = {{PV}-{NAS}},
	journal = {CoRR},
	author = {Wang, Zihao and Lin, Chen and Sheng, Lu and Yan, Junjie and Shao, Jing},
	year = {2020},
}

@article{sheng_unsupervised_2019,
	title = {Unsupervised {Bi}-directional {Flow}-based {Video} {Generation} from one {Snapshot}},
	volume = {abs/1903.00913},
	journal = {CoRR},
	author = {Sheng, Lu and Pan, Junting and Guo, Jiaming and Shao, Jing and Wang, Xiaogang and Loy, Chen Change},
	year = {2019},
}

@inproceedings{pan_actor-context-actor_2021,
	address = {Nashville, TN, USA},
	title = {Actor-{Context}-{Actor} {Relation} {Network} for {Spatio}-{Temporal} {Action} {Localization}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578296/},
	doi = {10.1109/CVPR46437.2021.00053},
	language = {en},
	urldate = {2022-08-26},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pan, Junting and Chen, Siyu and Shou, Mike Zheng and Liu, Yu and Shao, Jing and Li, Hongsheng},
	month = jun,
	year = {2021},
	pages = {464--474},
	file = {Pan et al. - 2021 - Actor-Context-Actor Relation Network for Spatio-Te.pdf:/Users/lucasjing/Zotero/storage/7ISIAC4B/Pan et al. - 2021 - Actor-Context-Actor Relation Network for Spatio-Te.pdf:application/pdf},
}
