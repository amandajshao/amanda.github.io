<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.6.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Jing Shao"><meta name=description content="Research Director"><link rel=alternate hreflang=en-us href=https://amandajshao.github.io/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.e16b071c3437dd38fe4bc6049c3c2fff.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><script src=https://identity.netlify.com/v1/netlify-identity-widget.js></script>
<link rel=alternate href=/index.xml type=application/rss+xml title="Jing Shao (邵婧)"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://amandajshao.github.io/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Jing Shao (邵婧)"><meta property="og:url" content="https://amandajshao.github.io/"><meta property="og:title" content="Jing Shao (邵婧)"><meta property="og:description" content="Research Director"><meta property="og:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="twitter:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2022-08-24T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebSite","potentialAction":{"@type":"SearchAction","target":"https://amandajshao.github.io/?q={search_term_string}","query-input":"required name=search_term_string"},"url":"https://amandajshao.github.io/"}</script><title>Jing Shao (邵婧)</title></head><body id=top data-spy=scroll data-offset=70 data-target=#navbar-main class=page-wrapper><script src=/js/wowchemy-init.min.613040fe4f2c0f007b4dcb64404201cb.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about data-target=#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts data-target=#posts><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#publications data-target=#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact data-target=#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><span class="js-widget-page d-none"></span><section id=sider class="home-section wg-slider carousel slide" data-ride=carousel data-interval=false><div class=home-section-bg></div><ol class=carousel-indicators><li data-target=#sider data-slide-to=0 class=active></li><li data-target=#sider data-slide-to=1></li><li data-target=#sider data-slide-to=2></li><li data-target=#sider data-slide-to=3></li><li data-target=#sider data-slide-to=4></li><li data-target=#sider data-slide-to=5></li><li data-target=#sider data-slide-to=6></li><li data-target=#sider data-slide-to=7></li><li data-target=#sider data-slide-to=8></li></ol><div class=carousel-inner><div class="carousel-item active" style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/dolphin.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>Dolphin: A General Video Interaction Platform Based on LLMs</h1><p class=hero-lead>Dolphin is a general video interaction platform based on large language models. Our team is trying to build a smart chatbot for video understanding, processing and generation.</p><p><a href=https://github.com/kaleido-lab/dolphin class="btn btn-light btn-lg mt-3">GitHub Repo</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/lamm.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</h1><p class=hero-lead>(1) We present the <strong>LAMM-Dataset</strong> and <strong>LAMM-Benchmark</strong>, which cover almost all high-level vision tasks for 2D and 3D vision. (2) We demonstrate how to construct instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. (3) We provide a primary but potential MLLM training framework optimized for modalities&rsquo; extension.</p><p><a href=https://github.com/OpenLAMM/LAMM class="btn btn-light btn-lg mt-3">GitHub Repo</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/intern.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>INTERN: A New Learning Paradigm Towards General Vision</h1><p class=hero-lead>Our new learning paradigm, INTERN, aims to systematically solve bottlenecks in recent <em>scenario-specifc</em> vision intelligence systems. The model being trained by INTERN will develop strong generalizability towards various vision tasks.</p><p><a href=../publication/shao-intern-2021 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/omnibenchmark.jpeg);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</h1><p class=hero-lead>Omni-Realm Benchmark (OmniBenchmark) is a <strong>diverse</strong> (21 semantic realm-wise datasets) and <strong>concise</strong> (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms.</p><p><a href=../publication/zhang-benchmarking-2022 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/xlearner.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation</h1><p class=hero-lead>Jointly learning from heterogeneous tasks and multiple data sources contributes to universal visual representation, leading to better transferring results of various downstream tasks.</p><p><a href=../publication/he-x-learner-2022 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/forgerynet.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</h1><p class=hero-lead>We host the <a href=https://competitions.codalab.org/competitions/33386 target=_blank rel=noopener>ForgeryNet Challenge 2021</a> in <a href=https://yinanhe.github.io/projects/forgerynet.html# target=_blank rel=noopener>ICCV 2021, The 3rd Workshop on Sensing, Understanding and Synthesizing Humans</a>.</p><p><a href=../publication/he-forgerynet-2021 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/mmekg.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities</h1><p class=hero-lead>MMEKG unifies different modalities of knowledge via events, which complement and disambiguate each other.</p><p><a href=../publication/ma-mmekg-2022 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/paie.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction</h1><p class=hero-lead>In this paper, we propose an effective yet efficient model PAIE for both sentence-level and document-level Event Argument Extraction (EAE), which also generalizes well when there is a lack of training data.</p><p><a href=../publication/ma-prompt-2022 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div><div class=carousel-item style=height:600px;background-image:url(https://amandajshao.github.io/media/sider/celebaspoof.png);background-repeat:no-repeat;background-position:50%;background-size:cover><div class="position-absolute d-flex w-100 h-100 justify-content-center align-items-center" style=-webkit-backdrop-filter:brightness(.5);backdrop-filter:brightness(.5)><div class="wg-hero dark container" style=margin-left:6rem;margin-right:6rem;text-align:left><h1 class=hero-title>CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations</h1><p class=hero-lead>CelebA-Spoof is a large-scale face anti-spoofing dataset that has 625,537 images from 10,177 subjects, which includes 43 rich attributes on face, illumination, environment and spoof types.</p><p><a href=../publication/vedaldi-celeba-spoof-2020 class="btn btn-light btn-lg mt-3">Read More</a></p></div></div></div></div><a class=carousel-control-prev href=#sider data-slide=prev><span class=carousel-control-prev-icon></span>
<span class=sr-only>Previous</span></a>
<a class=carousel-control-next href=#sider data-slide=next><span class=carousel-control-next-icon></span>
<span class=sr-only>Next</span></a></section><section id=about class="home-section wg-about"><div class=home-section-bg></div><div class=container><div class=row><div class="col-12 col-lg-4"><div id=profile><img class="avatar avatar-circle" width=270 height=270 src=/authors/admin/avatar_hu0831a317521287bc78ddc887a369a120_259525_270x270_fill_q75_lanczos_center.jpg alt="Jing Shao"><div class=portrait-title><h2>Jing Shao</h2><h3>Research Director</h3><h3><a href=https://www.sensetime.com target=_blank rel=noopener><span>SenseTime Group Ltd.</span></a></h3></div><ul class=network-icon aria-hidden=true><li><a href=/#contact aria-label=envelope><i class="fas fa-envelope big-icon"></i></a></li><li><a href="https://scholar.google.com.hk/citations?hl=zh-CN&amp;user=VU5ObUwAAAAJ" target=_blank rel=noopener aria-label=google-scholar><i class="ai ai-google-scholar big-icon"></i></a></li><li><a href=/uploads/jshao_cv_v2.pdf aria-label=cv><i class="ai ai-cv big-icon"></i></a></li></ul></div></div><div class="col-12 col-lg-8"><h1>Biography</h1><div class=article-style><p><span style=color:red><strong>[News!]</strong></span> Jing Shao will join <a href=https://www.shlab.org.cn/ target=_blank rel=noopener>Shanghai Artificial Intelligence Laboratory</a> as a research scientist focusing on multi-modal foundation models and their applications. <span style=color:red><strong>We are hiring full-time researchers working together on this project. I am also looking for talented students targeted to Master or Ph.D. degree, as well as interns. Please drop me an email if you are interested.</strong></span></p><p>She is currently a Research Director in <a href=https://www.sensetime.com target=_blank rel=noopener>SenseTime Group Limited</a>, and an Adjunct Ph.D. Supervisor at the <a href=https://www.sjtu.edu.cn/ target=_blank rel=noopener>Shanghai Jiao Tong University</a>. She is a Co-PI of <a href>S-Lab</a> in <a href=https://www.ntu.edu.sg/ target=_blank rel=noopener>Nanyang Technological University</a>. She received her Ph.D. (2016) in Electronic Engineering from <a href=http://www.cuhk.edu.hk/english/index.html target=_blank rel=noopener>The Chinese University of Hong Kong (CUHK)</a>, supervised by <a href=http://www.ee.cuhk.edu.hk/~xgwang/ target=_blank rel=noopener>Prof. Xiaogang Wang</a>, and work closely with <a href=https://www.mmlab-ntu.com/person/ccloy/index.html target=_blank rel=noopener>Prof. Chen Change Loy</a> and the <a href=http://mmlab.ie.cuhk.edu.hk/ target=_blank rel=noopener>Multimedia Lab (MMLab)</a> led by <a href=https://www.ie.cuhk.edu.hk/people/xotang.shtml target=_blank rel=noopener>Prof. Xiaoou Tang</a>.</p><p>Her research interests lies at the computer vision and multimedia, with focus on generating, perceiving, and understanding the 2D/3D visual world, for instance,</p><ul><li>Multimodal LLM,</li><li>General Vision Representation</li><li>Human-centric Understanding and Generation (Face, Body, Scene, and <em>etc</em>).</li></ul><p>She has published 40+ peer-reviewed articles (including 20 first/co-first author papers) in top-tier conferences and journals such as TPAMI, IJCV, ICML, ICLR, NeurIPS and CVPR, with 4700+ citations in Google Scholar. She serves as the reviewer of IJCV, T-PAMI, T-CSVT, T-MM, T-ITS, and CVIU, and reviewed CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI, IJCAI, and ACM MM.</p></div><div class=row></div></div></div></div></section><section id=posts class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>News</h1></div><div class="col-12 col-lg-8"><ul><li><strong>2023.06</strong>: I will join <a href=https://www.shlab.org.cn/ target=_blank rel=noopener>Shanghai Artificial Intelligence Laboratory</a> as a research scientist focusing on multi-modal foundation models and their applications. <span style=color:red>We are hiring full-time researchers working together on this project. I am also looking for talented students targeted to Master or Ph.D. degree, as well as interns. Please drop me an email if you are interested.</span></li><li><strong>2022.08</strong>: We are organizing ECCV 2022 (2nd) <a href=https://deeperaction.github.io/ target=_blank rel=noopener>DeeperAction Challenge and Workshop</a> on detailed video action understanding and anomaly recognition by introducing five new benchmarks.</li><li><strong>2022.08</strong>: We are hosting ECCV 2022 <a href=https://zhangyuanhan-ai.github.io/OmniBenchmark/ target=_blank rel=noopener>OmniBenchmark Challenge</a> on evaluation of pre-trained models.</li><li><strong>2022.07</strong>: <span style=color:red>Winner</span> of <a href=https://ai.google.com/research/rxr/habitat target=_blank rel=noopener>Challenge on RxR-Habitat Vision-and-Language Navigation Competition</a>.</li><li><strong>2022.07</strong>: Four papers accepted to <a href=https://eccv2022.ecva.net/ target=_blank rel=noopener>ECCV 2022</a>.</li><li><strong>2022.04</strong>: We have released <a href=https://opengvlab.shlab.org.cn/ target=_blank rel=noopener>OpenGVLab (书生)</a>.</li><li><strong>2022.04</strong>: One paper accepted to <a href=https://ijcai-22.org/ target=_blank rel=noopener>IJCAI 2022</a>.</li><li><strong>2022.03</strong>: Two papers accepted to <a href=https://www.2022.aclweb.org/ target=_blank rel=noopener>ACL 2022</a>.</li><li><strong>2021.11</strong>: <span style=color:red>A large-scale general vision paradigm <a href=https://opengvlab.shlab.org.cn/ target=_blank rel=noopener>INTERN</a> is released on <a href=https://arxiv.org/abs/2111.08687 target=_blank rel=noopener>Arxiv</a></span>.</li><li><strong>2021.10</strong>: We are organizing ICCV 2021 (2nd) <a href=https://sense-human.github.io/index_2021.html target=_blank rel=noopener>SenseHuman Workshop</a> on human sensing, understanding and synthesis.</li><li><strong>2021.10</strong>: We are organizing ICCV 2021 (1st) <a href=https://deeperaction.github.io/ target=_blank rel=noopener>DeeperAction Workshop</a> on detailed video action understanding by introducing three new benchmarks.</li><li><strong>2021.08</strong>: One paper accepted to <span style=color:red><strong>IEEE T-PAMI</strong></span>.</li><li><strong>2021.07</strong>: We are hosting <a href=https://competitions.codalab.org/competitions/33386 target=_blank rel=noopener>ForgeryNet - Face Forgery Analysis Challenge in ICCV 2021</a>.</li><li><strong>2021.04</strong>: Two papers accepted to <a href=https://cvpr2021.thecvf.com/ target=_blank rel=noopener>CVPR 2021</a> (One <span style=color:red><strong>Oral</strong></span> and one Poster).</li><li><strong>2020.09</strong>: We are hosting <a href=https://competitions.codalab.org/competitions/26210 target=_blank rel=noopener>CelebA-Spoof - Face Anti-Spoofing Challenge in ECCV 2020</a>.</li><li><strong>2020.08</strong>: <span style=color:red>Winner</span> of <a href=http://activity-net.org/challenges/2020/ target=_blank rel=noopener>ActivityNet Challenge on Spatio-temporal Action Localization (AVA)</a>.</li><li><strong>2020.07</strong>: 4th Place of <a href=https://www.kaggle.com/c/deepfake-detection-challenge target=_blank rel=noopener>DeepFake Detection Challenge (DFDC)</a>.</li><li><strong>2020.07</strong>: One paper accepted to <span style=color:red><strong>IJCV</strong></span>.</li><li><strong>2020.07</strong>: Three papers accepted to <a href=https://eccv2020.ecva.net/ target=_blank rel=noopener>ECCV 2020</a>.</li><li><strong>2019.09</strong>: Three papers: 1@<a href=https://iccv2019.thecvf.com/ target=_blank rel=noopener>ICCV 2019</a> on Text-image retrieval, 1@<a href=https://nips.cc/Conferences/2019 target=_blank rel=noopener>NeurIPS 2019</a> on Layout to image generation, 1@<a href=https://aaai.org/Conferences/AAAI-20/ target=_blank rel=noopener>AAAI 2020</a> on Dense point cloud completion.</li><li><strong>2019.06</strong>: <span style=color:green><strong>[Bug (invalid links) fixed]</strong></span> <a href=file:///Users/lucasjing/Library/Containers/com.tencent.xinWeChat/Data/Library/Application%20Support/com.tencent.xinWeChat/2.0b4.0.9/0055825b7948b58660b56956863332f3/Handoff/projects/CUHKcrowd_files/cuhk_crowd_dataset_files/CUHKcrowd_dataset_imgTrk_v2.txt>Baidu Disk links</a> of CUHK Crowd Dataset has been renewed!</li><li><strong>2019.04</strong>: Four papers accepted to <a href=https://cvpr2019.thecvf.com/ target=_blank rel=noopener>CVPR 2019</a> (one <span style=color:red><strong>Oral</strong></span> and three Posters)</li><li><strong>2018.07</strong>: Five papers: 3@<a href=https://eccv2018.ecva.net/ target=_blank rel=noopener>ECCV 2018</a>, 1@<a href=https://2022.acmmm.org/2018 target=_blank rel=noopener>ACM MM 2018</a>, 1@<a href=http://bmvc2018.org/ target=_blank rel=noopener>BMVC 2018</a>.</li><li><strong>2018.04</strong>: Three papers accepted to <a href=https://cvpr2018.thecvf.com/ target=_blank rel=noopener>CVPR 2018</a> (one <span style=color:red><strong>Oral</strong></span> and two Posters)</li><li><strong>2017.07</strong>: Two papers accepted to <a href=https://iccv2017.thecvf.com/ target=_blank rel=noopener>ICCV 2017</a>.</li><li><strong>2017.04</strong>: One paper accepted to <a href=https://cvpr2017.thecvf.com/ target=_blank rel=noopener>CVPR 2017</a></li><li><strong>2016.12</strong>: One paper accepted to <strong>IEEE TCSVT</strong>.</li><li><strong>2016.09</strong>: I am invited to participate in <a href=https://www.microsoft.com/en-us/research/blog/top-phd-students-gather-microsoft-research-asia-phd-forum-2016/ target=_blank rel=noopener>MSRA Ph.D. Forum 2016</a>.</li><li><strong>2016.06</strong>: Participate in <a href=http://cvpr2016.thecvf.com/program/doctoral_consortium target=_blank rel=noopener>CVPR 2016 Doctoral Consortium</a>.</li><li><strong>2016.06</strong>: One paper accepted to <strong>IEEE TCSVT</strong>.</li><li><strong>2016.04</strong>: One <span style=color:red><strong>Spotlight</strong></span> paper accepted to <a href=https://cvpr2016.thecvf.com/ target=_blank rel=noopener>CVPR 2016</a>.</li><li><strong>2015.04</strong>: One <span style=color:red><strong>Oral</strong></span> paper accepted to <a href=https://cvpr2015.thecvf.com/ target=_blank rel=noopener>CVPR 2015</a>.</li><li><strong>2014.04</strong>: One <span style=color:red><strong>Oral</strong></span> paper accepted to <a href=https://cvpr2014.thecvf.com/ target=_blank rel=noopener>CVPR 2014</a>.</li></ul></div></div></div></section><section id=working_experience class="home-section wg-experience"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Working Experience</h1></div><div class="col-12 col-lg-8"><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border exp-fill">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.sensetime.com target=_blank rel=noopener><img src=/media/icons/brands/sensetime-logo.svg width=56px height=56px alt="SenseTime Group Ltd." loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Research Director</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.sensetime.com target=_blank rel=noopener>SenseTime Group Ltd.</a></div><div class="text-muted exp-meta">Apr 2020 –
Present</div></div></div><div class=card-text><ul><li>Founded and led the General Vision Group to build general vision paradigm named INTERN based on multi-modal signals.</li><li>Acted as the project manager for Level-4 autonomonous driving</li><li>Led the research on visual perception for Level 2 & Level 4 autonomonous driving</li><li>2021 SenseTime’s Best Team Award.</li></ul></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.sensetime.com target=_blank rel=noopener><img src=/media/icons/brands/sensetime-logo.svg width=56px height=56px alt="SenseTime Group Ltd." loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Senior Research Manager</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.sensetime.com target=_blank rel=noopener>SenseTime Group Ltd.</a></div><div class="text-muted exp-meta">Apr 2018 –
Apr 2020</div></div></div><div class=card-text><ul><li>Led the Liveness Group which made the exponential growth in both accuracy and applicable usage of face anti-spoofing and face forgery detection.</li><li>The solutions were applied to more than 10 product lines for 200 customers that make cities safer and more efficient.</li><li>2019 SenseTime’s Best Team Award.</li></ul></div></div></div></div></div><div class="row experience"><div class="col-auto text-center flex-column d-none d-sm-flex"><div class="row h-50"><div class="col border-right">&nbsp;</div><div class=col>&nbsp;</div></div><div class=m-2><span class="badge badge-pill border">&nbsp;</span></div><div class="row h-50"><div class=col>&nbsp;</div><div class=col>&nbsp;</div></div></div><div class="col py-2"><div class=card><div class=card-body><div class="d-flex align-content-start"><div class="mr-2 mb-2"><a href=https://www.sensetime.com target=_blank rel=noopener><img src=/media/icons/brands/sensetime-logo.svg width=56px height=56px alt="SenseTime Group Ltd." loading=lazy></a></div><div><div class="section-subheading card-title exp-title text-muted my-0">Senior Researcher</div><div class="section-subheading card-title exp-company text-muted my-0"><a href=https://www.sensetime.com target=_blank rel=noopener>SenseTime Group Ltd.</a></div><div class="text-muted exp-meta">Nov 2016 –
Apr 2018</div></div></div><div class=card-text><ul><li>Founded and led the Intelligent Photo Group to build intelligent solutions on image and video recognition, especially for video surveilliance.</li></ul></div></div></div></div></div></div></div></div></section><section id=publications class="home-section wg-pages"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Recent Publications</h1></div><div class="col-12 col-lg-8"><div class="alert alert-note"><div>Quickly discover relevant content by <a href=./publication/>filtering publications</a>.</div></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuanhan Zhang</span>, <span>Qinghong Sun</span>, <span>Yichun Zhou</span>, <span>Zexin He</span>, <span>Zhenfei Yin</span>, <span>Kun Wang</span>, <span>Lu Sheng</span>, <span>Yu Qiao</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span></span>
(2022).
<a href=/publication/zhang-bamboo-2022/>Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.07845 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-bamboo-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ZhangYuanhan-AI/Bamboo target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://opengvlab.shlab.org.cn/bamboo/home target=_blank rel=noopener>Project</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jianing Teng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Wang Kun</span>, <span>Zhenfei Yin</span>, <span>Lu Sheng</span>, <span>Ziwei Liu</span>, <span>Yu Qiao</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></span>
(2022).
<a href=/publication/he-x-learner-2022/>X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2203.08764 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-x-learner-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuanhan Zhang</span>, <span>Zhenfei Yin</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span></span>
(2022).
<a href=/publication/zhang-benchmarking-2022/>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2207.07106 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-benchmarking-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Luya Wang</span>, <span>Feng Liang</span>, <span>Yangguang Li</span>, <span>Honggang Zhang</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span></span>
(2022).
<a href=/publication/wang-repre-2022/>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</a>.
International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ijcai.org/proceedings/2022/0200.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-repre-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.24963/ijcai.2022/200 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Dong An</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zun Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yangguang Li</span>, <span>Yi Wang</span>, <span>Yicong Hong</span>, <span>Yan Huang</span>, <span>Liang Wang</span>, <span>Jing Shao</span></span>
(2022).
<a href=/publication/an-1-st-2022/>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2206.11610 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/an-1-st-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mukai Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Meiqi Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xinze Li</span>, <span>Wenqi Sun</span>, <span>Kunquan Deng</span>, <span>Kun Wang</span>, <span>Aixin Sun</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i></span>
(2022).
<a href=/publication/ma-mmekg-2022/>MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities</a>.
Annual Meeting of the Association for Computational Linguistics: System Demonstrations (<strong>ACL Demo</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-demo.23.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-mmekg-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-demo.23 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Mukai Li</span>, <span>Meiqi Chen</span>, <span>Kun Wang</span>, <span>Jing Shao</span></span>
(2022).
<a href=/publication/ma-prompt-2022/>Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction</a>.
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (<strong>ACL Long Papers</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-long.466.pdf target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-prompt-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mayubo2333/PAIE target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-long.466 target=_blank rel=noopener>DOI</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Meiqi Chen</span>, <span>Yixin Cao</span>, <span>Kunquan Deng</span>, <span>Mukai Li</span>, <span>Kun Wang</span>, <span>Jing Shao</span>, <span>Yan Zhang</span></span>
(2022).
<a href=/publication/chen-ergo-2022/>ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.07434v1 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chen-ergo-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Haonan Qiu</span>, <span>Siyu Chen</span>, <span>Bei Gan</span>, <span>Kun Wang</span>, <span>Huafeng Shi</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span></span>
(2022).
<a href=/publication/qiu-few-shot-2022/>Few-shot Forgery Detection via Guided Adversarial Interpolation</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.05905 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/qiu-few-shot-2022/cite.bib>Cite</a></p></div><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yufeng Cui</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Feng Liang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yangguang Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Jing Shao</span></span>
(2022).
<a href=/publication/cui-democratizing-2022/>Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.05796 target=_blank rel=noopener>PDF</a>
<a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/cui-democratizing-2022/cite.bib>Cite</a>
<a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Sense-GVT/DeCLIP target=_blank rel=noopener>Project</a></p></div><div class=see-all><a href=/publication/>See all publications
<i class="fas fa-angle-right"></i></a></div></div></div></div></section><section id=contact class="home-section wg-contact"><div class=home-section-bg></div><div class=container><div class=row><div class="section-heading col-12 col-lg-4 mb-3 mb-lg-0 d-flex flex-column align-items-center align-items-lg-start"><h1 class=mb-0>Contact</h1></div><div class="col-12 col-lg-8"><ul class=fa-ul><li><i class="fa-li fas fa-envelope fa-2x" aria-hidden=true></i>
<span id=person-email><a href=mailto:amandajshao%20[at]%20gmail%20[dot]%20com>amandajshao [at] gmail [dot] com</a></span></li></ul></div></div></div></section></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Jing Shao. Last updated Aug 30, 2022.</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.2621b0c3d1130efc0b22bddd510b0848.js></script>
<script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/en/js/wowchemy.min.85290d887e7fcdd400ccb3ffb9bcd3e3.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>