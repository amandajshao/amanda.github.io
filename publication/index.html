<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Hugo Blox Builder 5.9.7"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><link rel=stylesheet href=/css/vendor-bundle.min.26c458e6907dc03073573976b7f4044e.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.5691485ce8e88de823038b8e3d90ec13.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><meta name=author content="Jing Shao"><meta name=description content="Research Scientist"><link rel=alternate hreflang=en-us href=https://amandajshao.github.io/publication/><link rel=canonical href=https://amandajshao.github.io/publication/><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_3.png><meta name=theme-color content="#1565c0"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:type" content="website"><meta property="og:site_name" content="Jing Shao (邵婧)"><meta property="og:url" content="https://amandajshao.github.io/publication/"><meta property="og:title" content="Publications | Jing Shao (邵婧)"><meta property="og:description" content="Research Scientist"><meta property="og:image" content="https://amandajshao.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png"><meta property="og:locale" content="en-us"><meta property="og:updated_time" content="2023-06-20T00:00:00+00:00"><link rel=alternate href=/publication/index.xml type=application/rss+xml title="Jing Shao (邵婧)"><title>Publications | Jing Shao (邵婧)</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=3a079e7dad19be978a318345a7749d34><script src=/js/wowchemy-init.min.9e4214442a7711d35691acd58f6f6361.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class="page-header header--fixed"><header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jing Shao (邵婧)</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>News</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span>
</a><a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span>
</a><a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><div class="universal-wrapper pt-3"><h1>Publications</h1></div><div class=universal-wrapper><div class=row><div class=col-lg-12><div class="form-row mb-4"><div class=col-auto><input type=search class="filter-search form-control form-control-sm" placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false></div><div class=col-auto><select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group=pubtype><option value=*>Type</option><option value=.pubtype-1>1</option><option value=.pubtype-2>2</option><option value=.pubtype-3>3</option></select></div><div class=col-auto><select class="pub-filters form-control form-control-sm" data-filter-group=year><option value=*>Date</option><option value=.year-2023>2023</option><option value=.year-2022>2022</option><option value=.year-2021>2021</option><option value=.year-2020>2020</option><option value=.year-2019>2019</option><option value=.year-2018>2018</option><option value=.year-2017>2017</option><option value=.year-2016>2016</option><option value=.year-2015>2015</option><option value=.year-2014>2014</option></select></div></div><div id=container-publications><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2023"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zeren Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Wei Li</span>, <span>Jianing Teng</span>, <span>Kun Wang</span>, <span>Jing Shao</span>, <span>Chen Change Loy</span>, <span>Lu Sheng</span>
</span>(2023).
<a href=/publication/siamese-detr/>Siamese DETR</a>.
CVPR.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2023/papers/Huang_Siamese_DETR_CVPR_2023_paper.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Zx55/SiameseDETR target=_blank rel=noopener>Code</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Ziyi Lin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal Contribution"></i>, <span>Xiatian Zhu</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span>
</span>(2022).
<a href=/publication/st-adapter/>ST-Adapter: Parameter-efficient Image-to-Video Transfer Learning</a>.
NeurIPS.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://proceedings.neurips.cc/paper_files/paper/2022/file/a92e9165b22d4456fc6d87236e04c266-Paper-Conference.pdf target=_blank rel=noopener>PDF
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/linziyi96/st-adapter target=_blank rel=noopener>Code</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuanhan Zhang</span>, <span>Qinghong Sun</span>, <span>Yichun Zhou</span>, <span>Zexin He</span>, <span>Zhenfei Yin</span>, <span>Kun Wang</span>, <span>Lu Sheng</span>, <span>Yu Qiao</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span>
</span>(2022).
<a href=/publication/zhang-bamboo-2022/>Bamboo: Building Mega-Scale Vision Dataset Continually with Human-Machine Synergy</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.07845 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-bamboo-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ZhangYuanhan-AI/Bamboo target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://opengvlab.shlab.org.cn/bamboo/home target=_blank rel=noopener>Project</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Gengshi Huang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jianing Teng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Wang Kun</span>, <span>Zhenfei Yin</span>, <span>Lu Sheng</span>, <span>Ziwei Liu</span>, <span>Yu Qiao</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>
</span>(2022).
<a href=/publication/he-x-learner-2022/>X-Learner: Learning Cross Sources and Tasks for Universal Visual Representation</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2203.08764 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-x-learner-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuanhan Zhang</span>, <span>Zhenfei Yin</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span>
</span>(2022).
<a href=/publication/zhang-benchmarking-2022/>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=http://arxiv.org/abs/2207.07106 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-benchmarking-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Luya Wang</span>, <span>Feng Liang</span>, <span>Yangguang Li</span>, <span>Honggang Zhang</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span>
</span>(2022).
<a href=/publication/wang-repre-2022/>RePre: Improving Self-Supervised Vision Transformer with Reconstructive Pre-training</a>.
International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ijcai.org/proceedings/2022/0200.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-repre-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.24963/ijcai.2022/200 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Dong An</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zun Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yangguang Li</span>, <span>Yi Wang</span>, <span>Yicong Hong</span>, <span>Yan Huang</span>, <span>Liang Wang</span>, <span>Jing Shao</span>
</span>(2022).
<a href=/publication/an-1-st-2022/>1st Place Solutions for RxR-Habitat Vision-and-Language Navigation Competition (CVPR 2022)</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2206.11610 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/an-1-st-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Mukai Li</span>, <span>Meiqi Chen</span>, <span>Kun Wang</span>, <span>Jing Shao</span>
</span>(2022).
<a href=/publication/ma-prompt-2022/>Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction</a>.
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (<strong>ACL Long Papers</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-long.466.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-prompt-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/mayubo2333/PAIE target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-long.466 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yubo Ma</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zehao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mukai Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yixin Cao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Meiqi Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xinze Li</span>, <span>Wenqi Sun</span>, <span>Kunquan Deng</span>, <span>Kun Wang</span>, <span>Aixin Sun</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>
</span>(2022).
<a href=/publication/ma-mmekg-2022/>MMEKG: Multi-modal Event Knowledge Graph towards Universal Representation across Modalities</a>.
Annual Meeting of the Association for Computational Linguistics: System Demonstrations (<strong>ACL Demo</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://aclanthology.org/2022.acl-demo.23.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-mmekg-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.18653/v1/2022.acl-demo.23 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Meiqi Chen</span>, <span>Yixin Cao</span>, <span>Kunquan Deng</span>, <span>Mukai Li</span>, <span>Kun Wang</span>, <span>Jing Shao</span>, <span>Yan Zhang</span>
</span>(2022).
<a href=/publication/chen-ergo-2022/>ERGO: Event Relational Graph Transformer for Document-level Event Causality Identification</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.07434v1 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chen-ergo-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Haonan Qiu</span>, <span>Siyu Chen</span>, <span>Bei Gan</span>, <span>Kun Wang</span>, <span>Huafeng Shi</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span>
</span>(2022).
<a href=/publication/qiu-few-shot-2022/>Few-shot Forgery Detection via Guided Adversarial Interpolation</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2204.05905 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/qiu-few-shot-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yufeng Cui</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Feng Liang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yangguang Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Jing Shao</span>
</span>(2022).
<a href=/publication/cui-democratizing-2022/>Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2203.05796 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/cui-democratizing-2022/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Sense-GVT/DeCLIP target=_blank rel=noopener>Project</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yangguang Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Feng Liang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Lichen Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yufeng Cui</span>, <span>Wanli Ouyang</span>, <span>Jing Shao</span>, <span>Fengwei Yu</span>, <span>Junjie Yan</span>
</span>(2022).
<a href=/publication/li-supervision-2022/>Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</a>.
International Conference on Learning Representations (<strong>ICLR</strong>), 2022.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://openreview.net/pdf?id=zq1iJkNk3uN" target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/li-supervision-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ruining Tang</span>, <span>Zhenyu Liu</span>, <span>Yangguang Li</span>, <span>Yiguo Song</span>, <span>Hui Liu</span>, <span>Qide Wang</span>, <span>Jing Shao</span>, <span>Guifang Duan</span>, <span>Jianrong Tan</span>
</span>(2022).
<a href=/publication/tang-task-balanced-2022/>Task-Balanced Distillation for Object Detection</a>.
<em>CoRR</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/tang-task-balanced-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Ziyi Lin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xiatian Zhu</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span>
</span>(2022).
<a href=/publication/pan-st-adapter-2022/>ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/2206.13559.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pan-st-adapter-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Hao Wang</span>, <span>Yangguang Li</span>, <span>Zhen Huang</span>, <span>Yong Dou</span>, <span>Lingpeng Kong</span>, <span>Jing Shao</span>
</span>(2022).
<a href=/publication/wang-sncse-2022/>SNCSE: Contrastive Learning for Unsupervised Sentence Embedding with Soft Negative Samples</a>.
<em>CoRR</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-sncse-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2022"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuanhan Zhang</span>, <span>Yichao Wu</span>, <span>Zhenfei Yin</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span>
</span>(2022).
<a href=/publication/zhang-robust-2022/>Robust Face Anti-Spoofing with Dual Probabilistic Modeling</a>.
<em>CoRR</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhang-robust-2022/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yinan He</span>, <span>Lu Sheng</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span>, <span>Zhaofan Zou</span>, <span>Zhizhi Guo</span>, <span>Shan Jiang</span>, <span>Curitis Sun</span>, <span>Guosheng Zhang</span>, <span>Keyao Wang</span>, <span>Haixiao Yue</span>, <span>Zhibin Hong</span>, <span>Wanguo Wang</span>, <span>Zhenyu Li</span>, <span>Qi Wang</span>, <span>Zhenli Wang</span>, <span>Ronghao Xu</span>, <span>Mingwen Zhang</span>, <span>Zhiheng Wang</span>, <span>Zhenhang Huang</span>, <span>Tianming Zhang</span>, <span>Ningning Zhao</span>
</span>(2021).
<a href=/publication/he-forgerynet-2021-1/>ForgeryNet - Face Forgery Analysis Challenge 2021: Methods and Results</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2112.08325 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-forgerynet-2021-1/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Teli Ma</span>, <span>Shijie Geng</span>, <span>Mengmeng Wang</span>, <span>Jing Shao</span>, <span>Jiasen Lu</span>, <span>Hongsheng Li</span>, <span>Peng Gao</span>, <span>Yu Qiao</span>
</span>(2021).
<a href=/publication/ma-simple-2021/>A Simple Long-Tailed Recognition Baseline via Vision-Language Model</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2111.14745 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ma-simple-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/gaopengcuhk/BALLAD target=_blank rel=noopener>Code</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zhao Zhong</span>, <span>Zichen Yang</span>, <span>Boyang Deng</span>, <span>Junjie Yan</span>, <span>Wei Wu</span>, <span>Jing Shao</span>, <span>Cheng-Lin Liu</span>
</span>(2021).
<a href=/publication/zhong-blockqnn-2021/>BlockQNN: Efficient Block-Wise Neural Network Architecture Generation</a>.
IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>IEEE T-PAMI</strong>), 2021.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhong-blockqnn-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TPAMI.2020.2969193 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yinan He</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Bei Gan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yichun Zhou</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Guojun Yin</span>, <span>Luchuan Song</span>, <span>Lu Sheng</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Ziwei Liu</span>
</span>(2021).
<a href=/publication/he-forgerynet-2021/>ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style=color:red><strong>Oral Presentation</strong></span>, 2021.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/He_ForgeryNet_A_Versatile_Benchmark_for_Comprehensive_Forgery_Analysis_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/he-forgerynet-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://yinanhe.github.io/projects/forgerynet.html#download target=_blank rel=noopener>Dataset
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR46437.2021.00434 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Mike Zheng Shou</span>, <span>Yu Liu</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span>
</span>(2021).
<a href=/publication/pan-actor-context-actor-2021/>Actor-Context-Actor Relation Network for Spatio-Temporal Action Localization</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2021.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content/CVPR2021/papers/Pan_Actor-Context-Actor_Relation_Network_for_Spatio-Temporal_Action_Localization_CVPR_2021_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pan-actor-context-actor-2021/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Siyu-C/ACAR-Net target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR46437.2021.00053 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2021"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Bowen Yang</span>, <span>Jing Zhang</span>, <span>Zhenfei Yin</span>, <span>Jing Shao</span>
</span>(2021).
<a href=/publication/yang-few-shot-2021/>Few-Shot Domain Expansion for Face Anti-Spoofing</a>.
<em>CoRR</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/yang-few-shot-2021/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuyang Qian</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Guojun Yin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution, Corresponding author"></i>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Zixuan Chen</span>, <span>Jing Shao</span>
</span>(2020).
<a href=/publication/vedaldi-thinking-2020/>Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590613.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-thinking-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58610-2_6 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Ronghao Guo</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Chen Lin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Chuming Li</span>, <span>Keyu Tian</span>, <span>Ming Sun</span>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Junjie Yan</span>
</span>(2020).
<a href=/publication/vedaldi-powering-2020/>Powering One-Shot Topological NAS with Stabilized Share-Parameter Proxy</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590613.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-powering-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58568-6_37 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Kun Yuan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Quanquan Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jing Shao</span>, <span>Junjie Yan</span>
</span>(2020).
<a href=/publication/vedaldi-learning-2020/>Learning Connectivity of Neural Networks from a Topological Perspective</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660732.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-learning-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58589-1_44 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yuanhan Zhang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Zhenfei Yin</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Yidong Li</span>, <span>Guojun Yin</span>, <span>Junjie Yan</span>, <span>Jing Shao</span>, <span>Ziwei Liu</span>
</span>(2020).
<a href=/publication/vedaldi-celeba-spoof-2020/>CelebA-Spoof: Large-Scale Face Anti-spoofing Dataset with Rich Annotations</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570069.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/vedaldi-celeba-spoof-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ZhangYuanhan-AI/CelebA-Spoof target=_blank rel=noopener>Dataset
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.youtube.com/watch?v=A7XjSg5srvI&amp;t=4s" target=_blank rel=noopener>Video
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-58610-2_5 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Siyu Chen</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Junting Pan</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Guanglu Song</span>, <span>Manyuan Zhang</span>, <span>Hao Shao</span>, <span>Ziyi Lin</span>, <span>Jing Shao</span>, <span>Hongsheng Li</span>, <span>Yu Liu</span>
</span>(2020).
<a href=/publication/chen-1-st-2020/>1st place solution for AVA-Kinetics Crossover in AcitivityNet Challenge 2020</a>.
<em>CoRR</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/2006.09116 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/chen-1-st-2020/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Lu Sheng</span>, <span>Junting Pan</span>, <span>Jiaming Guo</span>, <span>Jing Shao</span>, <span>Chen Change Loy</span>
</span>(2020).
<a href=/publication/sheng-high-quality-2020/>High-Quality Video Generation from Static Structural Annotations</a>.
International Journal of Computer Vision (<strong>IJCV</strong>), 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://link.springer.com/article/10.1007/s11263-020-01334-x target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/sheng-high-quality-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/junting/seg2vid target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/s11263-020-01334-x target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Minghua Liu</span>, <span>Lu Sheng</span>, <span>Sheng Yang</span>, <span>Jing Shao</span>, <span>Shi-Min Hu</span>
</span>(2020).
<a href=/publication/liu-morphing-2020/>Morphing and Sampling Network for Dense Point Cloud Completion</a>.
AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>), 2020.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://ojs.aaai.org/index.php/AAAI/article/view/6827 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-morphing-2020/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Colin97/MSN-Point-Cloud-Completion target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://drive.google.com/drive/folders/1oYOT8fTUyj6_db9f4cYiKFqP9LPEG5nH target=_blank rel=noopener>Dataset
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1609/aaai.v34i07.6827 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2020"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zihao Wang</span>, <span>Chen Lin</span>, <span>Lu Sheng</span>, <span>Junjie Yan</span>, <span>Jing Shao</span>
</span>(2020).
<a href=/publication/wang-pv-nas-2020/>PV-NAS: Practical Neural Architecture Search for Video Recognition</a>.
<em>CoRR</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-pv-nas-2020/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Xihui Liu</span>, <span>Guojun Yin</span>, <span>Jing Shao</span>, <span>Xiaogang Wang</span>, <span>Hongsheng Li</span>
</span>(2019).
<a href=/publication/liu-learning-2019/>Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis</a>.
Advances in Neural Information Processing Systems (<strong>NeurIPS</strong>), 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1910.06809.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-learning-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/xh-liu/CC-FPSE target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://drive.google.com/file/d/1m3JKSUotm6sRImak_qjwBMtMtd037XeK/view target=_blank rel=noopener>Slides</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zihao Wang</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Xihui Liu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Hongsheng Li</span>, <span>Lu Sheng</span>, <span>Junjie Yan</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span>
</span>(2019).
<a href=/publication/wang-camp-2019/>CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval</a>.
IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>), 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_CAMP_Cross-Modal_Adaptive_Message_Passing_for_Text-Image_Retrieval_ICCV_2019_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-camp-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/ZihaoWang-CV/CAMP_iccv19 target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICCV.2019.00586 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Junting Pan</span>, <span>Chengyu Wang</span>, <span>Xu Jia</span>, <span>Jing Shao</span>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Junjie Yan</span>, <span>Xiaogang Wang</span>
</span>(2019).
<a href=/publication/pan-video-2019/>Video Generation From Single Semantic Label Map</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Pan_Video_Generation_From_Single_Semantic_Label_Map_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/pan-video-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/junting/seg2vid target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00385 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Guojun Yin</span>, <span>Bin Liu</span>, <span>Lu Sheng</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Nenghai Yu</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span>
</span>(2019).
<a href=/publication/yin-semantics-2019/>Semantics Disentangling for Text-To-Image Generation</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style=color:red><strong>Oral Presentation</strong></span>, 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Semantics_Disentangling_for_Text-To-Image_Generation_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/yin-semantics-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00243 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Xihui Liu</span>, <span>Zihao Wang</span>, <span>Jing Shao</span>, <span>Xiaogang Wang</span>, <span>Hongsheng Li</span>
</span>(2019).
<a href=/publication/liu-improving-2019/>Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2019.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Improving_Referring_Expression_Grounding_With_Cross-Modal_Attention-Guided_Erasing_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-improving-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/xh-liu/CM-Erase-REG target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00205 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Guojun Yin</span>, <span>Lu Sheng</span>, <span>Bin Liu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Nenghai Yu</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span>
</span>(2019).
<a href=/publication/yin-context-2019/>Context and Attribute Grounded Dense Captioning</a>.
<em>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Yin_Context_and_Attribute_Grounded_Dense_Captioning_CVPR_2019_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/yin-context-2019/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2019.00640 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2019"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Lu Sheng</span>, <span>Junting Pan</span>, <span>Jiaming Guo</span>, <span>Jing Shao</span>, <span>Xiaogang Wang</span>, <span>Chen Change Loy</span>
</span>(2019).
<a href=/publication/sheng-unsupervised-2019/>Unsupervised Bi-directional Flow-based Video Generation from one Snapshot</a>.
<em>CoRR</em>.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/sheng-unsupervised-2019/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yongcheng Liu</span>, <span>Lu Sheng</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Junjie Yan</span>, <span>Shiming Xiang</span>, <span>Chunhong Pan</span>
</span>(2018).
<a href=/publication/liu-multi-label-2018/>Multi-Label Image Classification via Knowledge Distillation from Weakly-Supervised Detection</a>.
ACM International Conference on Multimedia (<strong>MM</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1809.05884 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-multi-label-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/Yochengliu/MLIC-KD-WSD target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1145/3240508.3240567 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Xihui Liu</span>, <span>Hongsheng Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Coresponding author"></i>, <span>Jing Shao</span>, <span>Dapeng Chen</span>, <span>Xiaogang Wang</span>
</span>(2018).
<a href=/publication/ferrari-show-2018/>Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1803.08314 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ferrari-show-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-01267-0_21 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yu Liu</span>, <span>Guanglu Song</span>, <span>Jing Shao</span>, <span>Xiao Jin</span>, <span>Xiaogang Wang</span>
</span>(2018).
<a href=/publication/ferrari-transductive-2018/>Transductive Centroid Projection for Semi-supervised Large-Scale Recognition</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Yu_Liu_Transductive_Centroid_Projection_ECCV_2018_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ferrari-transductive-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-01228-1_5 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Pengze Liu</span>, <span>Xihui Liu</span>, <span>Junjie Yan</span>, <span>Jing Shao</span>
</span>(2018).
<a href=/publication/liu-localization-2018/>Localization Guided Learning for Pedestrian Attribute Recognition</a>.
British Machine Vision Conference (<strong>BMVC</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1808.09102 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-localization-2018/cite.bib>Cite</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zhao Zhong</span>, <span>Junjie Yan</span>, <span>Wei Wu</span>, <span>Jing Shao</span>, <span>Cheng-Lin Liu</span>
</span>(2018).
<a href=/publication/zhong-practical-2018/>Practical Block-Wise Neural Network Architecture Generation</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style=color:red><strong>Oral Presentation</strong></span>, 2018.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhong-practical-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2018.00257 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Yu Liu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Fangyin Wei</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Lu Sheng</span>, <span>Junjie Yan</span>, <span>Xiaogang Wang</span>
</span>(2018).
<a href=/publication/liu-exploring-2018/>Exploring Disentangled Feature Representation Beyond Face Identification</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Exploring_Disentangled_Feature_CVPR_2018_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-exploring-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2018.00222 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Lu Sheng</span>, <span>Ziyi Lin</span>, <span>Jing Shao</span>, <span>Xiaogang Wang</span>
</span>(2018).
<a href=/publication/sheng-avatar-net-2018/>Avatar-Net: Multi-scale Zero-Shot Style Transfer by Feature Decoration</a>.
IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/sheng-avatar-net-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/LucasSheng/avatar-net target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://lucassheng.github.io/avatar-net/ target=_blank rel=noopener>Project
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2018.00860 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Guojun Yin</span>, <span>Lu Sheng</span>, <span>Bin Liu</span>, <span>Nenghai Yu</span>, <span>Xiaogang Wang</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Chen Change Loy</span>
</span>(2018).
<a href=/publication/ferrari-zoom-net-2018/>Zoom-Net: Mining Deep Feature Interactions for Visual Relationship Recognition</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/abs/1803.08314 target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ferrari-zoom-net-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-01219-9_20 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Dapeng Chen</span>, <span>Hongsheng Li</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Xihui Liu</span>, <span>Yantao Shen</span>, <span>Jing Shao</span>, <span>Zejian Yuan</span>, <span>Xiaogang Wang</span>
</span>(2018).
<a href=/publication/ferrari-improving-2018/>Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association</a>.
European Conference on Computer Vision (<strong>ECCV</strong>), 2018.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Dapeng_Chen_Improving_Deep_Visual_ECCV_2018_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/ferrari-improving-2018/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1007/978-3-030-01270-0_4 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Zhongdao Wang</span>, <span>Luming Tang</span>, <span>Xihui Liu</span>, <span>Zhuliang Yao</span>, <span>Shuai Yi</span>, <span>Jing Shao</span>, <span>Junjie Yan</span>, <span>Shengjin Wang</span>, <span>Hongsheng Li</span>, <span>Xiaogang Wang</span>
</span>(2017).
<a href=/publication/wang-orientation-2017/>Orientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-identification</a>.
IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2017.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Orientation_Invariant_Feature_ICCV_2017_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/wang-orientation-2017/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICCV.2017.49 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Xihui Liu</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Haiyu Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Maoqing Tian</span>, <span>Lu Sheng</span>, <span>Jing Shao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Corresponding author"></i>, <span>Shuai Yi</span>, <span>Junjie Yan</span>, <span>Xiaogang Wang</span>
</span>(2017).
<a href=/publication/liu-hydraplus-net-2017/>HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</a>.
IEEE International Conference on Computer Vision (<strong>ICCV</strong>), 2017.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://arxiv.org/pdf/1709.09930.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/liu-hydraplus-net-2017/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/xh-liu/HydraPlus-Net target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/ICCV.2017.46 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Haiyu Zhao</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Maoqing Tian</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span>Shuyang Sun</span>, <span>Jing Shao</span>, <span>Junjie Yan</span>, <span>Shuai Yi</span>, <span>Xiaogang Wang</span>, <span>Xiaoou Tang</span>
</span>(2017).
<a href=/publication/zhao-spindle-2017/>Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion</a>.
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2017.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/zhao-spindle-2017/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yokattame/SpindleNet target=_blank rel=noopener>Code
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://github.com/yokattame/SpindleNet target=_blank rel=noopener>Dataset
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2017.103 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2017"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jing Shao</span>, <span>Chen Change Loy</span>, <span>Xiaogang Wang</span>
</span>(2017).
<a href=/publication/shao-learning-2017/>Learning Scene-Independent Group Descriptors for Crowd Understanding</a>.
IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE T-CSVT</strong>), 2017.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/shao-learning-2017/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TCSVT.2016.2539878 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2017"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jing Shao</span>, <span>Chen Change Loy</span>, <span>Kai Kang</span>, <span>Xiaogang Wang</span>
</span>(2017).
<a href=/publication/shao-crowded-2017/>Crowded Scene Understanding by Deeply Learned Volumetric Slices</a>.
IEEE Transactions on Circuits and Systems for Video Technology (<strong>IEEE T-CSVT</strong>), 2016.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/shao-crowded-2017/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/TCSVT.2016.2593647 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2016"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jing Shao</span>, <span>Chen Change Loy</span>, <span>Kai Kang</span>, <span>Xiaogang Wang</span>
</span>(2016).
<a href=/publication/shao-slicing-2016/>Slicing Convolutional Neural Network for Crowd Video Understanding</a>.
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style=color:red><strong>Spotlight</strong></span>, 2016.<p><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/shao-slicing-2016/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2016.606 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2015"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jing Shao</span>, <span>Kai Kang</span>, <span>Chen Change Loy</span>, <span>Xiaogang Wang</span>
</span>(2015).
<a href=/publication/shao-deeply-2015/>Deeply learned attributes for crowded scene understanding</a>.
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style=color:red><strong>Oral Presentation</strong></span>, 2015.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_cvpr_2015/papers/Shao_Deeply_Learned_Attributes_2015_CVPR_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/shao-deeply-2015/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2015.7299097 target=_blank rel=noopener>DOI</a></p></div></div><div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2014"><div class="pub-list-item view-citation" style=margin-bottom:1rem><i class="far fa-file-alt pub-icon" aria-hidden=true></i>
<span class="article-metadata li-cite-author"><span>Jing Shao</span>, <span>Chen Change Loy</span>, <span>Xiaogang Wang</span>
</span>(2014).
<a href=/publication/shao-scene-independent-2014/>Scene-Independent Group Profiling in Crowd</a>.
IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), <span style=color:red><strong>Oral Presentation</strong></span>, 2014.<p><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://openaccess.thecvf.com/content_cvpr_2014/papers/Shao_Scene-Independent_Group_Profiling_2014_CVPR_paper.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header btn-sm js-cite-modal" data-filename=/publication/shao-scene-independent-2014/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header btn-sm" href=https://doi.org/10.1109/CVPR.2014.285 target=_blank rel=noopener>DOI</a></p></div></div></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Jing Shao. Last updated Aug 30, 2022.</p><p class=powered-by>Published with <a href="https://hugoblox.com/?utm_campaign=poweredby" target=_blank rel=noopener>Hugo Blox Builder</a> — the free, <a href=https://github.com/HugoBlox/hugo-blox-builder target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.391d344a129df56f7ad674c2c2ed04e8.js></script><script src=https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script><script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js type=module></script><script src=/en/js/wowchemy.min.7f5ebaff62ae468cff8bb3dd1337bb9b.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.9c0e895144aef5a693008b5c5d450147.js type=module></script></body></html>